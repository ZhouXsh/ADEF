<h1 align='center'>éŸ³é¢‘é©±åŠ¨çš„æƒ…æ„Ÿèåˆè¯´è¯è§†é¢‘ç”Ÿæˆçš„ç ”ç©¶</h1>
<h2 align='center'>ADEF: Research on Audio-Driven
Emotion-Fused Talking Video Generation</h2>

## ğŸ“– é¡¹ç›®ä»‹ç»

## ğŸ§³ æ¡†æ¶

![Inference Pipeline](assets/imgs/pipeline.png)

## âš™ï¸ æ¨¡å‹éƒ¨ç½²

**ç³»ç»Ÿé…ç½®:**

Ubuntu:

- è®­ç»ƒ&æµ‹è¯•ï¼šUbuntu 20.04, CUDA 12.1
- è®­ç»ƒ&æµ‹è¯•ï¼šGPUs: RTX 4090

Windows:

- å»ºè®® Windows 11, CUDA 12.1
- å»ºè®® GPUs: RTX 4060 Laptop 8GB VRAM GPU

**åˆ›å»ºç¯å¢ƒ:**

```bash
# 1. Create base environment
conda create -n adef python=3.10.16 -y
conda activate adef 

# 2. Install requirements
pip install -r requirements.txt

# 3. Install ffmpeg
sudo apt-get update  
sudo apt-get install ffmpeg -y
```

## ğŸ’ ä¸‹è½½æ¨¡å‹æ£€æŸ¥ç‚¹

ç¡®ä¿ç³»ç»Ÿå®‰è£…äº†[git-lfs](https://git-lfs.com)ã€‚å¹¶å°†æ¨¡å‹æ£€æŸ¥ç‚¹ä¸‹è½½åˆ°`pretrained_weights`ç›®å½•ã€‚

### 1. ä¸‹è½½æˆ‘ä»¬çš„ ADEF æ£€æŸ¥ç‚¹

```bash
git lfs install
git clone https://huggingface.co/ZhouXSh/ADEF
```

### 2. ä¸‹è½½éŸ³é¢‘ç¼–ç å™¨æ£€æŸ¥ç‚¹

We suport two types of audio encoders, including [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h), and [hubert-chinese](https://huggingface.co/TencentGameMate/chinese-hubert-base).

Run the following commands to download [hubert-chinese](https://huggingface.co/TencentGameMate/chinese-hubert-base) pretrained weights:

```bash
git lfs install
git clone https://huggingface.co/TencentGameMate/chinese-hubert-base
```

To get the [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h) pretrained weights, run the following commands:

```bash
git lfs install
git clone https://huggingface.co/facebook/wav2vec2-base-960h
```

### 3. Download LivePortraits checkpoints

```bash
# !pip install -U "huggingface_hub[cli]"
huggingface-cli download KwaiVGI/LivePortrait --local-dir pretrained_weights --exclude "*.git*" "README.md" "docs"
```

Refering to [Liveportrait](https://github.com/KwaiVGI/LivePortrait/tree/main) for more download methods.

### 4. é¢„è®­ç»ƒæ¨¡å‹ï¼ˆ`pretrained_weights` ç›®å½•ï¼‰ç»“æ„

`pretrained_weights`ç›®å½•æœ€ç»ˆæ˜¯ä»¥ä¸‹å±‚çº§å…³ç³»ï¼š

```text
./pretrained_weights/
â”œâ”€â”€ ADEF
â”‚   â”œâ”€â”€ audio2emo
â”‚   â”‚   â””â”€â”€ audio2emo.pth
â”‚   â”œâ”€â”€ emo_classifier
â”‚   â”‚   â””â”€â”€ emo_level_classifier.pth
â”‚   â”œâ”€â”€ emo_enhancer
â”‚   â”‚   â””â”€â”€ emo_enhancer.pth
â”‚   â”œâ”€â”€ motion_generator
â”‚   â”‚   â””â”€â”€ motion_generator.pt
â”‚   â””â”€â”€ motion_template
â”‚       â””â”€â”€ motion_template.pkl
â”œâ”€â”€ insightface                                                                                                                                                 
â”‚   â””â”€â”€ models                                                                                                                                                  
â”‚       â””â”€â”€ buffalo_l                                                                                                                                           
â”‚           â”œâ”€â”€ 2d106det.onnx                                                                                                                                   
â”‚           â””â”€â”€ det_10g.onnx   
â”œâ”€â”€ liveportrait
â”‚   â”œâ”€â”€ base_models
â”‚   â”‚   â”œâ”€â”€ appearance_feature_extractor.pth
â”‚   â”‚   â”œâ”€â”€ motion_extractor.pth
â”‚   â”‚   â”œâ”€â”€ spade_generator.pth
â”‚   â”‚   â””â”€â”€ warping_module.pth
â”‚   â”œâ”€â”€ landmark.onnx
â”‚   â””â”€â”€ retargeting_models
â”‚       â””â”€â”€ stitching_retargeting_module.pth
â”œâ”€â”€ TencentGameMate:chinese-hubert-base
â”‚   â”œâ”€â”€ chinese-hubert-base-fairseq-ckpt.pt
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ gitattributes
â”‚   â”œâ”€â”€ preprocessor_config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â””â”€â”€ README.md
â””â”€â”€ wav2vec2-base-960h               
    â”œâ”€â”€ config.json                  
    â”œâ”€â”€ feature_extractor_config.json
    â”œâ”€â”€ model.safetensors
    â”œâ”€â”€ preprocessor_config.json
    â”œâ”€â”€ pytorch_model.bin
    â”œâ”€â”€ README.md
    â”œâ”€â”€ special_tokens_map.json
    â”œâ”€â”€ tf_model.h5
    â”œâ”€â”€ tokenizer_config.json
    â””â”€â”€ vocab.json
```

> [!NOTE]
> Windowsä¸­çš„æ–‡ä»¶å¤¹â€œTencentGameMate:chinese hubert baseâ€åº”é‡å‘½åä¸ºâ€œchinese hubert baseâ€ã€‚

## ğŸš€ æ¨ç†

### 0. ä¸€äº›é‡è¦å‚æ•°

| å‚æ•°   | å«ä¹‰ | å–å€¼èŒƒå›´            |
|--------|------|------------------|
| -r   | è¢«é©±åŠ¨çš„äººè„¸å›¾åƒ   | å›¾åƒè·¯å¾„ï¼Œå­—ç¬¦ä¸²       |
| -a   | é©±åŠ¨éŸ³é¢‘   | éŸ³é¢‘è·¯å¾„ï¼Œå­—ç¬¦ä¸²         |
| -e   | æƒ…æ„Ÿç±»å‹   | angryã€contemptã€disgustedã€fearã€happyã€neutralã€sadã€surprised   |
| --cfg_scale   | CFGæƒé‡   | å»ºè®®>1 & <4ï¼Œæ•°å­—   |
| --output_dir   | ç”Ÿæˆè§†é¢‘ä¿å­˜ç›®å½•   | ç›®å½•è·¯å¾„ï¼Œå­—ç¬¦ä¸²  |
| --use_emo_enhancer  | æ˜¯å¦ä½¿ç”¨æƒ…æ„Ÿå¢å¼º   | True or False   |
| --enhance_level   | æƒ…æ„Ÿå¢å¼ºçš„çº§åˆ«   | 1ã€2ã€3 æ•°å­—   |
| --use_emo_analyzer   | æ˜¯å¦å¯¹éŸ³é¢‘è¿›è¡Œæƒ…æ„Ÿåˆ†æ   | True or False    |
| --device_id   | GPUç¼–å·   | çœ‹æ‚¨æœ‰å‡ ä¸ªGPUï¼Œæ•°å­—   |
### 1. ä½¿ç”¨å‘½ä»¤è¡Œæ¨ç†

å¯ä»¥æ·»åŠ ä¸Šè¿°çš„å‚æ•°

```python
python inference.py
```

æ‚¨å¯ä»¥æ›´æ”¹cfg_scaleä»¥è·å¾—ä¸åŒè¡¨æƒ…å’Œå¤´éƒ¨å§¿åŠ¿çš„ç»“æœã€‚

### 2. ä½¿ç”¨web demoæ¨ç†

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨ç½‘é¡µæ¼”ç¤º:

```python
python app.py
```

æ¼”ç¤ºå°†è¿è¡Œåœ¨http://127.0.0.1:7862.

## âš“ï¸ ä½¿ç”¨è‡ªå·±çš„æ•°æ®è®­ç»ƒæ¨¡å‹

æœ¬æ–¹æ³•ä½¿ç”¨[MEAD](https://github.com/uniBruce/Mead)è¿›è¡Œè®­ç»ƒã€‚

æ•°æ®è¦æ±‚ï¼šäººç±»è¯´è¯çš„é¢éƒ¨è§†é¢‘ã€æƒ…æ„Ÿç±»å‹æ ‡ç­¾ã€æƒ…æ„Ÿå¼ºåº¦ç­‰çº§æ ‡ç­¾ã€‚

æ•°æ®å‡†å¤‡æ—¶ï¼Œåº”è¯¥æŒ‰ç…§ä»¥ä¸‹ç»“æ„å±‚æ¬¡å’Œå‘½åè§„åˆ™ï¼š

```text
./dataset/
â””â”€â”€ MEAD
    â””â”€â”€ videos
        â”œâ”€â”€ M003
        â”œâ”€â”€ M005
            â””â”€â”€front
                â”œâ”€â”€angry
                    â”œâ”€â”€level_1
                        â”œâ”€â”€M005_front_angry_level_1_001.mp4
                        â”œâ”€â”€M005_front_angry_level_1_002.mp4
                        â”œâ”€â”€......
                        â””â”€â”€M005_front_angry_level_1_029.mp4
                    â”œâ”€â”€level_2
                    â””â”€â”€level_3
                â”œâ”€â”€contempt
                â”œâ”€â”€disgusted
                â”œâ”€â”€fear
                â”œâ”€â”€happy
                â”œâ”€â”€neutral
                â”œâ”€â”€sad
                â””â”€â”€surprised
        â”œâ”€â”€ M007
        â”œâ”€â”€ ...
        â””â”€â”€ W040
```

### 1. å‡†å¤‡è®­ç»ƒä¸æµ‹è¯•æ•°æ®

ä½¿ç”¨æ‚¨è‡ªå·±çš„æ•°æ®é›†è·¯å¾„æ›´æ”¹â€œ01_extract_gt_motions.pyâ€å’Œâ€œ01_extract_dit_motionsâ€ä¸­çš„â€œroot_dirâ€ï¼Œç„¶åè¿è¡Œä»¥ä¸‹å‘½ä»¤ä»¥ç”Ÿæˆè®­ç»ƒå’ŒéªŒè¯æ•°æ®ï¼š

```bash
cd src/prepare_data
python 01_extract_gt_motions.py
python 05_extract_audio.py
python 02_divide_dataset.py
pyhton 03_merge_gt_motions.py
python 04_generate_template.py
```

è®­ç»ƒæƒ…æ„Ÿå¢å¼ºæ¨¡å—ï¼Œéœ€è¦é¢å¤–æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼ˆéœ€è¦ç¡®ä¿è¿åŠ¨ç”Ÿæˆå™¨å·²å®Œæˆè®­ç»ƒï¼‰ï¼š
```bash
python 01_extract_dit_motions.py
python 03_merge_dit_motions.py
```

è®­ç»ƒéŸ³é¢‘æƒ…æ„Ÿè¯†åˆ«æ¨¡å—ï¼Œéœ€è¦é¢å¤–æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼ˆéœ€è¦ç”¨åˆ°[emotion2vec](https://github.com/ddlBoJack/emotion2vec) ï¼‰ï¼š
```bash
python 06_extract_emotion2vec.py
```

### 2. è®­ç»ƒ

é¦–å…ˆï¼Œè®­ç»ƒæƒ…æ„Ÿåˆ†ç±»å™¨ï¼Œä»¥ç”¨äºè®¡ç®—æƒ…æ„ŸæŸå¤±ï¼š

```bash
python train_emoClassifier.py
```

å…¶æ¬¡ï¼Œè®­ç»ƒæˆ‘ä»¬çš„è¿åŠ¨ç”Ÿæˆå™¨ã€‚å¯ä»¥æ›´æ”¹å…¶ä¸­çš„å‚æ•°ï¼Œä»¥ä¸ªæ€§åŒ–è®­ç»ƒï¼š

```bash
python train.py
```

å†æ¬¡ï¼Œè®­ç»ƒæƒ…æ„Ÿå¢å¼ºæ¨¡å—ã€‚éœ€è¦åœ¨è¿åŠ¨ç”Ÿæˆå™¨çš„åŸºç¡€ä¸Šè¿›è¡Œï¼š
```bash
python train_emoEnhancer.py
```

æœ€åï¼Œå¦‚æœ‰éœ€è¦ï¼Œå¯ä»¥è®­ç»ƒéŸ³é¢‘æƒ…æ„Ÿåˆ†ç±»å™¨ï¼š
```bash
python train_audio2emotion.py
```
å®éªŒç»“æœä½äºï¼š `experiments/`.

## ğŸ¤ æ„Ÿè°¢

We would like to thank the contributors to the 
[JoyVASA](https://github.com/KwaiVGI/LivePortrait),
[emotion2vec](https://github.com/ddlBoJack/emotion2vec), [LivePortrait](https://github.com/KwaiVGI/LivePortrait), [Open Facevid2vid](https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis), [InsightFace](https://github.com/deepinsight/insightface), [X-Pose](https://github.com/IDEA-Research/X-Pose), [DiffPoseTalk](https://github.com/DiffPoseTalk/DiffPoseTalk), [Hallo](https://github.com/fudan-generative-vision/hallo), [wav2vec 2.0](https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec), [Chinese Speech Pretrain](https://github.com/TencentGameMate/chinese_speech_pretrain), [Q-Align](https://github.com/Q-Future/Q-Align), [Syncnet](https://github.com/joonson/syncnet_python), and [VBench](https://github.com/Vchitect/VBench) repositories, for their open research and extraordinary work.
