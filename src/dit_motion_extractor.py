# coding: utf-8
import torch
torch.backends.cudnn.benchmark = True # disable CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR warning

import cv2; cv2.setNumThreads(0); cv2.ocl.setUseOpenCL(False)
import numpy as np
import os
import os.path as osp
import tyro
import subprocess
from rich.progress import track

from .config.argument_config import ArgumentConfig
from .config.inference_config import InferenceConfig
from .utils.camera import get_rotation_matrix
from .utils.io import dump
from .utils.helper import remove_suffix
from .utils.rprint import rlog as log


import os.path as osp
import os
import pickle
import numpy as np
import cv2
import torch
import yaml
import math
import librosa
import torch.nn.functional as F
from rich.progress import track

from .utils.timer import Timer
from .utils.helper import load_model, concat_feat
from .utils.camera import headpose_pred_to_degree, get_rotation_matrix
from .utils.retargeting_utils import calc_eye_close_ratio, calc_lip_close_ratio
from .config.inference_config import InferenceConfig
from .utils.rprint import rlog as log
from .utils.filter import smooth_
import os.path as osp
import os
import pickle
import numpy as np
import cv2
import torch
import yaml
import math
import librosa
import torch.nn.functional as F
from rich.progress import track

from .utils.helper import load_model
from .utils.camera import get_rotation_matrix
from .config.inference_config import InferenceConfig
from .utils.rprint import rlog as log
from .utils.filter import smooth_

'''
joyvasaçš„DiT  
ç”¨äºè®­ç»ƒæ—¶æå–motionçš„Motion Extractor
zxs 20250323
'''

# æ£€æŸ¥ffmpegæ˜¯å¦å­˜åœ¨
def fast_check_ffmpeg():
    try:
        subprocess.run(["ffmpeg", "-version"], capture_output=True, check=True)
        return True
    except:
        return False

# è¿”å›æ€»å‚æ•°kwargsä¸­ï¼Œtarget_classéƒ¨åˆ†çš„å‚æ•°å­—æ®µ
def partial_fields(target_class, kwargs):
    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})

# coding: utf-8

class DiTMotionExtractor(object):
    def __init__(self, inference_cfg: InferenceConfig):
        self.inference_cfg = inference_cfg
        self.device_id = inference_cfg.device_id
        if inference_cfg.flag_force_cpu:
            self.device = 'cpu'
        else:
            try:
                if torch.backends.mps.is_available():
                    self.device = 'mps'
                else:
                    self.device = 'cuda:' + str(self.device_id)
            except:
                self.device = 'cuda:' + str(self.device_id)
        
        model_config = yaml.load(open(inference_cfg.models_config, 'r'), Loader=yaml.SafeLoader)

        # Motion Genertor     è¿åŠ¨ç”Ÿæˆå™¨
        self.motion_generator, self.motion_generator_args = load_model(inference_cfg.checkpoint_MotionGenerator, model_config, self.device, 'motion_generator')
        log(f'Load motion_generator from {osp.realpath(inference_cfg.checkpoint_MotionGenerator)} done.')
        self.n_motions = self.motion_generator_args.n_motions                  # å•ä¸ªéŸ³é¢‘å­åºåˆ—ï¼ˆç‰‡æ®µï¼‰çš„å¸§æ•°
        self.n_prev_motions = self.motion_generator_args.n_prev_motions
        self.fps = self.motion_generator_args.fps
        self.audio_unit = 16000. / self.fps     # num of samples per frame     æ¯å¸§æ ·æœ¬æ•°
        self.n_audio_samples = round(self.audio_unit * self.n_motions)
        self.pad_mode = self.motion_generator_args.pad_mode
        self.use_indicator = self.motion_generator_args.use_indicator
        self.templete_dict = pickle.load(open(inference_cfg.motion_template_path, 'rb'))
    
    # ï¼ˆæ ¹æ®éŸ³é¢‘ï¼‰ç”Ÿæˆè¿åŠ¨åºåˆ—ï¼ï¼ï¼
    def gen_motion_sequence(self, args, suffix=".pkl"): 
       # preprocess audio    å¤„ç†éŸ³é¢‘
        log(f"start loading audio from {args.audio}")
        audio, _ = librosa.load(args.audio, sr=16000, mono=True)  # ä»ç»™å®šè·¯å¾„åŠ è½½éŸ³é¢‘ï¼Œè®¾ç½®é‡‡æ ·ç‡ä¸º 16,000 Hzï¼Œmono=Trueè¡¨ç¤ºå°†éŸ³é¢‘è½¬æ¢ä¸ºå•å£°é“
        # audio shape: (æ€»é‡‡æ ·æ—¶é•¿, )
        log(f"audio loaded! {audio.shape}")
        if isinstance(audio, np.ndarray):       # (n_samples,)   
            audio = torch.from_numpy(audio).to(self.device)         # è½¬æˆtorchå¼ é‡ç±»å‹  [n_samples]    eg:[170528]
        assert audio.ndim == 1, 'Audio must be 1D tensor.'          # ç¡®ä¿éŸ³é¢‘æ˜¯ä¸€ä¸ªä¸€ç»´å¼ é‡ï¼ˆå³å•é€šé“éŸ³é¢‘ï¼‰
        log(f"loading audio from: {args.audio}")
        audio = F.pad(audio, (1280, 640), "constant", 0)     # F.pad: å¯¹éŸ³é¢‘è¿›è¡Œå¡«å……ï¼Œå·¦ä¾§å¡«å…… 1280 ä¸ªæ ·æœ¬ï¼Œå³ä¾§å¡«å…… 640 ä¸ªæ ·æœ¬ã€‚å¡«å……å€¼ä¸º 0ã€‚  eg: [170528] -> [172448]


        # crop audio into n_subdivision according to n_motions   
        # æ ¹æ®n_motionsï¼ˆå¸§/ç‰‡æ®µæ•°ï¼‰å°†éŸ³é¢‘è£å‰ªä¸ºn_subdivisionï¼ˆç‰‡æ®µæ•°ï¼‰ä»½
        clip_len = int(len(audio) / 16000 * self.fps)       # len(audio) / 16000ä¸ºéŸ³é¢‘æ—¶é•¿ï¼›clip_lenä¸ºæ€»å¸§æ•°     eg:269
        stride = self.n_motions                   # æ­¥é•¿ï¼ˆæ¯ä¸ªéŸ³é¢‘ç‰‡æ®µçš„å¸§æ•°ï¼‰
        if clip_len <= self.n_motions:
            n_subdivision = 1                               # æ€»å¸§æ•°å°äºæ­¥é•¿ï¼Œåªåˆ†æˆä¸€ä»½
        else:
            n_subdivision = math.ceil(clip_len / stride)    # æŒ‰æ­¥é•¿åˆ†æˆn_subdivisionä»½      eg:3

        # padding  å¡«å……
        n_padding_audio_samples = self.n_audio_samples * n_subdivision - len(audio)  # è®¡ç®—éŸ³é¢‘éœ€è¦å¡«å……çš„æ ·æœ¬æ•°é‡ï¼Œä»¥ä¿è¯éŸ³é¢‘é•¿åº¦ä¸ n_subdivision å¯¹åº”çš„è¿åŠ¨ç”Ÿæˆæ ·æœ¬æ•°é‡åŒ¹é…   eg:  n_padding_audio_samples = 19552
        n_padding_frames = math.ceil(n_padding_audio_samples / self.audio_unit)           # éœ€è¦å¡«å……çš„å¸§æ•°
        if n_padding_audio_samples > 0:          # éœ€è¦å¡«å……çš„æ ·æœ¬æ•°é‡   eg: 19552
            if self.pad_mode == 'zero':               # ä½¿ç”¨ 0 å¡«å……
                padding_value = 0
            elif self.pad_mode == 'replicate':        # å¤åˆ¶æœ€åä¸€ä¸ªéŸ³é¢‘æ ·æœ¬å¡«å……
                padding_value = audio[-1]
            else:
                raise ValueError(f'Unknown pad mode: {self.pad_mode}')
            audio = F.pad(audio, (0, n_padding_audio_samples), value=padding_value)     #  172448 + 19552 = 192000 = self.n_audio_samples * n_subdivision
            # å…¶ä¸­ n_audio_samplesï¼šn_motions=100å¸§å¯¹åº”çš„é‡‡æ ·æ•°   ï¼›  n_subdivisionï¼šåˆ†æˆäº†å¤šå°‘ä¸ªn_motions=100å¸§
            # å·¦ä¾§å¡«å……0ä¸ªæ ·æœ¬ï¼Œå³ä¾§å¡«å…… n_padding_audio_samples ä¸ªæ ·æœ¬ã€‚å¡«å……å€¼ä¸º padding_valueã€‚  shapeï¼š(n_samples,)

        # generate motions
        coef_list = []
        for i in range(0, n_subdivision):              # éŸ³é¢‘çš„ä»½æ•°ï¼ˆå­åºåˆ—ï¼‰ï¼Œæ¯ä¸€ä»½åŒ…å« stride = n_motions=100å¸§ã€‚
            start_idx = i * stride                     # è¿™ä»½ èµ·å§‹ å¸§çš„ç´¢å¼•
            end_idx = start_idx + self.n_motions       # è¿™ä»½ ç»“æŸ å¸§çš„ç´¢å¼•
            indicator = torch.ones((1, self.n_motions)).to(self.device) if self.use_indicator else None  # æŒ‡ç¤ºå™¨ï¼ˆå…¨ 1 çš„å¼ é‡ï¼‰ï¼Œç”¨äºåœ¨æœ€åä¸€ä¸ªå­åºåˆ—ä¸Šæ ‡è®°å¡«å……éƒ¨åˆ†ã€‚(1, self.n_motions=100)
            if indicator is not None and i == n_subdivision - 1 and n_padding_frames > 0:
                indicator[:, -n_padding_frames:] = 0     # æŒ‡ç¤ºå™¨ï¼ˆçœŸå®éŸ³é¢‘ä¸ºTrueï¼Œå¡«å……éƒ¨åˆ†ä¸º0ï¼‰ï¼Œç”¨äºåœ¨æœ€åä¸€ä¸ªå­åºåˆ—ä¸Šæ ‡è®° å¡«å……çš„éƒ¨åˆ†ã€‚
            audio_in = audio[round(start_idx * self.audio_unit):round(end_idx * self.audio_unit)].unsqueeze(0)  # audio_in: æ ¹æ®éŸ³é¢‘çš„å­åºåˆ—ç´¢å¼•æå–éŸ³é¢‘ç‰‡æ®µï¼Œå¹¶å¢åŠ ä¸€ä¸ªç»´åº¦ä»¥å½¢æˆ (1, n_samples_in_subdivision) çš„å½¢çŠ¶ã€‚
            # shape: [1, self.n_audio_samples = audio_len / n_subdivision]   eg:[1, 64000]
  
            if i == 0:    # ç¬¬ä¸€ä¸ªç‰‡æ®µæ²¡æœ‰å…ˆå‰å¸§çš„æŒ‡å¯¼
                # è°ƒç”¨ motion_generator.sample ç”Ÿæˆè¿åŠ¨ç‰¹å¾ã€‚è¿”å›çš„ motion_feat æ˜¯ç”Ÿæˆçš„è¿åŠ¨ç‰¹å¾ï¼Œnoise æ˜¯å™ªå£°ï¼Œprev_audio_feat æ˜¯å…ˆå‰çš„éŸ³é¢‘ç‰¹å¾ã€‚
                motion_feat, noise, prev_audio_feat = self.motion_generator.sample(audio_in,        # (1, n_motion=100ï¼ˆå³frame per seq), n_features=73) - æ¯ä¸ªå­åºåˆ—ç”Ÿæˆçš„è¿åŠ¨ç‰¹å¾
                                                                        indicator=indicator, cfg_mode=args.cfg_mode,
                                                                        cfg_cond=args.cfg_cond, cfg_scale=args.cfg_scale,
                                                                        dynamic_threshold=0)
                # å…¶ä¸­motion_featï¼š(N=1, L=100, motion_feat_dim=73)   ç”Ÿæˆçš„è¿åŠ¨ç‰¹å¾ï¼ˆå»å™ªçš„ç»“æœï¼‰
                # noise:           (N=1, L=100, motion_feat_dim=73)  éšæœºå™ªå£°ï¼ˆå»å™ªçš„å¼€å§‹ï¼‰
                # prev_audio_feat: (N=1, L=100, feature_dim=256)     å½“å‰ç‰‡æ®µæå–çš„éŸ³é¢‘ç‰¹å¾ã€‚
            else:
                motion_feat, noise, prev_audio_feat = self.motion_generator.sample(audio_in,
                                                                        prev_motion_feat, prev_audio_feat, noise,     # éœ€è¦æ·»åŠ å…ˆå‰çš„ è¿åŠ¨ç‰¹å¾ï¼ŒéŸ³é¢‘ç‰¹å¾ï¼Œå™ªå£°
                                                                        indicator=indicator, cfg_mode=args.cfg_mode,
                                                                        cfg_cond=args.cfg_cond, cfg_scale=args.cfg_scale,
                                                                        dynamic_threshold=0)
            prev_motion_feat = motion_feat[:, -self.n_prev_motions:].clone()        # copyä¸´è¿‘çš„n_prev_motions=0å¸§ï¼Œå…ˆå‰çš„è¿åŠ¨ç‰¹å¾  (N=1, L=10, motion_feat_dim=73)
            prev_audio_feat = prev_audio_feat[:, -self.n_prev_motions:]             # copyï¼Œä½œä¸ºå…ˆå‰çš„éŸ³é¢‘ç‰¹å¾                      (N=1, L=10, feature_dim=256)

            motion_coef = motion_feat         #  è¿åŠ¨ç³»æ•° "coefficient"ï¼ˆç³»æ•°ï¼‰  (N=1, L=100, motion_feat_dim=73)
            if i == n_subdivision - 1 and n_padding_frames > 0:         # æœ€åä¸€æ®µéŸ³é¢‘åºåˆ—
                motion_coef = motion_coef[:, :-n_padding_frames]  # delete padded frames   åˆ é™¤å³ä¾§æ–°å¢çš„n_padding_frameså¸§  (N=1, L - n_padding_frames, motion_feat_dim=73)
            coef_list.append(motion_coef)   # len = 3
            motion_coef = torch.cat(coef_list, dim=1)                # (1, n_framesè§†é¢‘æ€»å¸§æ•°, n_features=73) - æ•´ä¸ªè¿åŠ¨åºåˆ—çš„è¿åŠ¨ç³»æ•°      n_frames = n_motions * n_subdivision
            # motion_coef = self.reformat_motion(args, motion_coef)

        motion_coef = motion_coef.squeeze() #.cpu().numpy().astype(np.float32)     # å»é™¤å¼ é‡ä¸­æ‰€æœ‰å°ºå¯¸ä¸º1çš„ç»´åº¦ã€‚(n_frames, n_features=70)
        motion_list = []              # è¿åŠ¨åˆ—è¡¨
        for idx in track(range(motion_coef.shape[0]), description='ğŸš€Generating Motion Sequence...', total=motion_coef.shape[0]):    # æ€»å¸§æ•°
            # æŒ‰ç…§æ¨¡æ¿å­—å…¸ä¸­çš„æ ‡å‡†å·®å’Œå‡å€¼è¿›è¡Œåå½’ä¸€åŒ–ï¼ˆä» 0~1 åˆ°å„è‡ªçš„èŒƒå›´ï¼‰
            exp = motion_coef[idx][:63].cpu() * self.templete_dict["std_exp"] + self.templete_dict["mean_exp"]    # [63]
            scale = motion_coef[idx][63:64].cpu() * (self.templete_dict["max_scale"] - self.templete_dict["min_scale"]) + self.templete_dict["min_scale"]   # [1]
            t = motion_coef[idx][64:67].cpu() * (self.templete_dict["max_t"] - self.templete_dict["min_t"]) + self.templete_dict["min_t"]    # [3]
            pitch = motion_coef[idx][67:68].cpu() * (self.templete_dict["max_pitch"] - self.templete_dict["min_pitch"]) + self.templete_dict["min_pitch"]   # [1]
            yaw = motion_coef[idx][68:69].cpu() * (self.templete_dict["max_yaw"] - self.templete_dict["min_yaw"]) + self.templete_dict["min_yaw"]   # [1]
            roll = motion_coef[idx][69:70].cpu() * (self.templete_dict["max_roll"] - self.templete_dict["min_roll"]) + self.templete_dict["min_roll"]   # [1]

            R = get_rotation_matrix(pitch, yaw, roll)                    # æ—‹è½¬çŸ©é˜µ
            R = R.reshape(1, 3, 3).cpu().numpy().astype(np.float32)           # (1, 3, 3)     1ä»£è¡¨åªæœ‰ä¸€å¼ å›¾ç‰‡    3*3çš„æ—‹è½¬çŸ©é˜µ
            
            exp = exp.reshape(1, 21, 3).cpu().numpy().astype(np.float32)      # (1, 21, 3)    21ä¸ªè¡¨æƒ…å…³é”®ç‚¹     
            scale = scale.reshape(1, 1).cpu().numpy().astype(np.float32)      # (1, 1)
            t = t.reshape(1, 3).cpu().numpy().astype(np.float32)              # (1, 3)         x,y,zä¸‰ä¸ªåæ ‡
            pitch = pitch.reshape(1, 1).cpu().numpy().astype(np.float32)      # (1, 1)         æ—‹è½¬å¯¹å…¨å±€ï¼ˆæ•´ä¸ªå¤´éƒ¨ï¼‰æœ‰æ•ˆï¼Œå› æ­¤åªæœ‰ä¸€ä¸ªå€¼
            yaw = yaw.reshape(1, 1).cpu().numpy().astype(np.float32)          # (1, 1)
            roll = roll.reshape(1, 1).cpu().numpy().astype(np.float32)        # (1, 1)
            
            motion_list.append({"exp": exp, "scale": scale, "R": R, "t": t, "pitch": pitch, "yaw": yaw, "roll": roll})
        tgt_motion = {'n_frames': motion_coef.shape[0], 'output_fps': 25, 'motion': motion_list}    # å¸§æ•°ã€‚å¸§ç‡ã€‚è¿åŠ¨å‚æ•°åˆ—è¡¨ã€‚

        if args.is_smooth_motion:      # å¹³æ»‘è¿åŠ¨
            tgt_motion = smooth_(tgt_motion, method="ema")      # ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰æ–¹æ³•å¯¹è¿åŠ¨è¿›è¡Œå¹³æ»‘
        wfp_template = remove_suffix(args.audio) + '_DiT' + suffix     # xxx.pkl
        dump(wfp_template, tgt_motion)          # ä¿å­˜æ–‡ä»¶
    
# ï¼ˆå¤„ç†è®­ç»ƒè§†é¢‘ï¼‰ç”Ÿæˆè¾“å…¥è§†é¢‘çš„ è¿åŠ¨æ¨¡ç‰ˆ
def make_motion_templete(args, driving_audio, suffix=".pkl", gpu_id=0): 
    os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)       # è®©è¯¥è¿›ç¨‹åªçœ‹åˆ°æŒ‡å®š GPU
    # print(f"Process {os.getpid()} using GPU {gpu_id} for video {driving_audio}")

    wfp_template = remove_suffix(driving_audio) + '_DiT' + suffix    # xxx.pkl
    if os.path.exists(wfp_template):  # å·²å¤„ç†
        # log(f"{driving_audio}motion generated ...")
        return

    # configs
    args.audio = driving_audio    # è§†é¢‘çš„ç»å¯¹è·¯å¾„
    inference_cfg = partial_fields(InferenceConfig, args.__dict__)      # æ¨ç†å‚æ•°

    # ffmpeg
    ffmpeg_dir = os.path.join(os.getcwd(), "ffmpeg")
    if osp.exists(ffmpeg_dir):
        os.environ["PATH"] += (os.pathsep + ffmpeg_dir)
    if not fast_check_ffmpeg():
        raise ImportError( "FFmpeg is not installed. Please install FFmpeg (including ffmpeg and ffprobe) before running this script. https://ffmpeg.org/download.html")

    try:
        # feature_extract
        motion_extractor = DiTMotionExtractor(
            inference_cfg=inference_cfg
        )
        motion_extractor.gen_motion_sequence(args, suffix=suffix)
    except Exception as e:
        print(f"Exception in motion extractor: {e}")