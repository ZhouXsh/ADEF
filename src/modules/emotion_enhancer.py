import logging
from random import randint
import sys
import torch
import torch.nn as nn
import os
import pickle
import numpy as np
import torch.optim as optim
from torch.utils import data
from tensorboardX import SummaryWriter
sys.path.append(os.path.dirname(os.path.abspath("../")))
from src.dataset import infinite_data_loader
from src.scheduler import GradualWarmupScheduler
from src.modules.common import PositionalEncoding

torch.backends.cudnn.benchmark = True # disable CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR warning

import cv2; cv2.setNumThreads(0); cv2.ocl.setUseOpenCL(False)
import os.path as osp
 
from rich.progress import track

from src.config.argument_config import ArgumentConfig
from src.config.inference_config import InferenceConfig
from src.utils.camera import get_rotation_matrix
from src.utils.video import images2video, add_audio_to_video
from src.utils.io import load_image_rgb, resize_to_limit
from src.utils.helper import mkdir, basename, dct2device
from src.utils.rprint import rlog as log
# from src.ADEF_wrapper import ADEFWrapper
import tyro
from src.modules.emotion_level_classifier import EmotionTransformer as Classifier

def partial_fields(target_class, kwargs):
    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})


emo_list = ['angry', 'contempt', 'disgusted', 'fear', 'happy', 'neutral', 'sad', 'surprised']
emo_label = ['ang',  'con',  'dis',  'fea',  'hap',  'neu',  'sad',  'sur']

# åŒåˆ†å¸ƒ
class DiT_Emo_Dataset(data.Dataset):
    def __init__(self, root_dir='/mnt/disk2/zhouxishi/JoyVASA/src/prepare_data', gt_motion_filename="front_motions.pkl", dit_motion_filename="front_dit_motions.pkl"):
        self.template_dict = pickle.load(open('/mnt/disk2/zhouxishi/JoyVASA/src/my_prepare/joyvasa_motion_template.pkl', 'rb'))
        self.gt_motion_data = pickle.load(open(os.path.join(root_dir, gt_motion_filename), "rb"))
        self.dit_motion_data = pickle.load(open(os.path.join(root_dir, dit_motion_filename), "rb"))
        print("load all motion data done...")
        self.eps = 1e-9
        
        txt_path = os.path.join(root_dir, "front_train.txt")
        with open(txt_path, "r", encoding="utf-8") as file:
            lines = [line.strip() for line in file.readlines()]
            self.all_data = [{
                "video_name": line,
                "audio_name": line[:-4]+'.wav'
            } for line in lines]
        self.all_num = 100

    def __len__(self):
        return len(self.all_data)
    
    def __getitem__(self, index):
        metadata = self.all_data[index] 
        # /W037/front/angry/level_3/W037_front_angry_level_3_034.mp4
        emotype = metadata['audio_name'].split('/')[-3]   # angry
        emo_index = torch.tensor(emo_list.index(emotype), dtype=torch.long)  # (1,)

        level_ = int(metadata['audio_name'].split('/')[-2].split('_')[-1])-1   # 0~2
        emo_level = torch.tensor(level_, dtype=torch.long)  # (1,)
    
        template_dict = self.template_dict         # åŒåˆ†å¸ƒ

        gt_motions = self.gt_motion_data[metadata["audio_name"]]
        dit_motions = self.dit_motion_data[metadata["audio_name"]]

        min_frames = min(gt_motions['n_frames'], dit_motions['n_frames'])
        
        gt_list, dit_list = [],[]
        for i in range(min_frames):
            gt_motion_exp = gt_motions['motion'][i]['exp'] * 1.5
            dit_motion_exp = dit_motions['motion'][i]['exp']           # (1,21,3)

            normalized_exp11 = (gt_motion_exp.flatten() - template_dict["mean_exp"]) / (template_dict["std_exp"] + self.eps)
            normalized_exp22 = (dit_motion_exp.flatten() - template_dict["mean_exp"]) / (template_dict["std_exp"] + self.eps)

            gt_motion_exp = torch.tensor(normalized_exp11, dtype=torch.float32)           # (63)
            dit_motion_exp = torch.tensor(normalized_exp22, dtype=torch.float32)          # (63)

            gt_list.append(gt_motion_exp)
            dit_list.append(dit_motion_exp)

        gt_motion_exps =  torch.stack(gt_list, dim=0)      # (min_frames, 63)
        dit_motion_exps =  torch.stack(dit_list, dim=0)    # (min_frames, 63)

        while gt_motion_exps.shape[0] < self.all_num + 2:
            gt_motion_exps = torch.cat([gt_motion_exps, gt_motion_exps], dim=0)
            dit_motion_exps = torch.cat([dit_motion_exps, dit_motion_exps], dim=0)

        end_frame = randint(self.all_num, gt_motion_exps.shape[0] - 1)
        start_frame = end_frame - self.all_num

        dit_prev = dit_motion_exps[start_frame:end_frame]
        gt_prev = gt_motion_exps[start_frame:end_frame]

        return dit_prev, gt_prev, emo_index, emo_level

class EmotionTransformer(nn.Module):
    def __init__(self, input_dim=63, emotion_dim=8, embed_dim=512, num_heads=8, ff_dim=512*4, num_layers=6):
        super(EmotionTransformer, self).__init__()
        
        self.embed_dim = embed_dim
        
        # è¿åŠ¨å‚æ•°åµŒå…¥å±‚ (B, L, 63) -> (B, L, D)
        self.motion_embedding = nn.Linear(input_dim, embed_dim)
        
        # ä½ç½®ç¼–ç 
        # self.positional_encoding = nn.Parameter(torch.zeros(1, 100, embed_dim))  # å‡è®¾æœ€å¤§ L=100
        self.PE = PositionalEncoding(self.embed_dim)

        # Transformer Encoder
        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim, batch_first=True)
        self.encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)

        # Transformer Decoder
        decoder_layers = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim, batch_first=True)
        self.decoder = nn.TransformerDecoder(decoder_layers, num_layers=num_layers)

        self.emotion_embedding = nn.Embedding(emotion_dim, embed_dim)  # (B, 1, 8) -> (B, 1, D)
        self.emotion_level_embedding = nn.Embedding(3, embed_dim) 

        # èåˆæƒ…æ„Ÿç±»åˆ«+ç­‰çº§ï¼ˆæ‹¼æ¥åæ˜ å°„ï¼‰
        self.emotion_fusion = nn.Linear(embed_dim * 2, embed_dim)

        # è¾“å‡ºå±‚ (B, 1, D) -> (B, 1, 63)
        self.output_layer = nn.Linear(embed_dim, input_dim)

    def forward(self, motion_seq, emotion, level):
        """
        motion_seq: (B, L, 63)  æ— è¡¨æƒ…è¿åŠ¨åºåˆ—
        emotion: (B,)  One-hot æƒ…æ„Ÿä¿¡æ¯
        """

        emotion = self.emotion_embedding(emotion)    # (B, )  ->  (B, D)
        level = self.emotion_level_embedding(level)  # (B, )  ->  (B, D)

        emotion_embed = torch.cat([emotion, level], dim=-1)        # (B, D*2)
        emotion_embed = self.emotion_fusion(emotion_embed).unsqueeze(1)  # (B, 1, D)

        # è¿åŠ¨å‚æ•°åµŒå…¥ + ä½ç½®ç¼–ç 
        motion_embed = self.motion_embedding(motion_seq)  # (B, L, D)
        motion_embed = self.PE(motion_embed)

        # ç»è¿‡ Transformer Encoder
        encoded_features = self.encoder(motion_embed)  # (B, L, D)

        # ç»è¿‡ Transformer Decoder
        decoded_output = self.decoder(encoded_features, emotion_embed)  # (B, 1, D)

        # è¾“å‡ºè¿åŠ¨å‚æ•° (B, 1, D) -> (B, 1, 63)
        output = self.output_layer(decoded_output)

        return output

def transf_train():
    # è®­ç»ƒè¶…å‚æ•°
    num_epochs = 120000
    learning_rate = 1e-4

    warm_iter = 12000
    decay_iter = 120000
    batch_size = 64
    # åŒåˆ†å¸ƒ
    log_dir = f'./0511_64_120000_gt15å¢å¼º'
    
    writer = SummaryWriter(log_dir)   # è·¯å¾„

    # åˆå§‹åŒ–æ¨¡å‹
    device = torch.device("cuda:3" if torch.cuda.is_available() else "cpu")
    model = EmotionTransformer().to(device)

    # 0417 æƒ…æ„Ÿåˆ†ç±»å™¨    0426é«˜dimä¸”åŒåˆ†å¸ƒ
    emo_classifier = Classifier().to(device)
    emo_classifier.load_state_dict(torch.load(f'/mnt/disk2/zhouxishi/JoyVASA/pretrained_weights/ADEF/emo_classifier/emo_level_classifier.pth', map_location=device))
    emo_classifier.eval()
    criterion = torch.nn.CrossEntropyLoss()

    # å®šä¹‰æŸå¤±å‡½æ•°
    mse_loss = nn.MSELoss()  # ä¸»è¦æŸå¤±ï¼Œçº¦æŸè¿åŠ¨å‚æ•°
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    after_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, decay_iter, learning_rate * 0.02)
    scheduler = GradualWarmupScheduler(optimizer, 1, warm_iter, after_scheduler)

    train_dataset = DiT_Emo_Dataset()
    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    train_loader = infinite_data_loader(train_loader)   # å°†æ•°æ®åŠ è½½å™¨ï¼ˆtrain_loaderï¼‰è½¬æ¢ä¸ºä¸€ä¸ªæ— é™å¾ªç¯çš„è¿­ä»£å™¨

    model.train()
    loss_log = {
        'total_loss': [],
        'mse_loss': []
        ,'emo_loss':[]
        ,'level_loss': []
    }

    for epoch in range(num_epochs):
        dit_prev, _, gt_prev, _, emo_index, emo_level = next(train_loader)
        dit_prev = dit_prev.to(device)     # ï¼ˆB,100,63ï¼‰
        # dit_cur = dit_cur.to(device)      # ï¼ˆB,100,63ï¼‰
        emo_index = emo_index.to(device)     # ï¼ˆB,ï¼‰
        gt_prev = gt_prev.to(device)       # ï¼ˆB,100,63ï¼‰
        # gt_cur = gt_cur.to(device)       # ï¼ˆB,100,63ï¼‰
        emo_level = emo_level.to(device)

        optimizer.zero_grad()

        pred = model(dit_prev, emo_index, emo_level)     #   (B, 1, 63)

        B, L, _ = dit_prev.shape
        dit_prev = dit_prev + pred.expand(-1, L, -1)   # (B, 1, 63)  -> (B, L, 63)

        # === è®¡ç®—å„ä¸ªæŸå¤± ===
        loss_mse = mse_loss(dit_prev, gt_prev)  # (B, 100, 63)

        pred_emo, pred_level = emo_classifier(dit_prev)   # (N,100,63)  -> (N,8)
        loss_emo = criterion(pred_emo, emo_index)
        loss_level = criterion(pred_level, emo_level)

        # === è®¡ç®—æ€»æŸå¤± ===
        total_loss = loss_mse + loss_emo + loss_level

        # åå‘ä¼ æ’­
        total_loss.backward()
        optimizer.step()

        # Inside your training loop, after computing the losses:
        # Logging - Append current batch losses
        loss_log['total_loss'].append(total_loss.mean().item())
        loss_log['mse_loss'].append(loss_mse.mean().item())
        loss_log['emo_loss'].append(loss_emo.mean().item())
        loss_log['level_loss'].append(loss_emo.mean().item())

        # Create description string for logging
        description = f'Iter: {epoch}\t Train loss: [Total: {np.mean(loss_log["total_loss"]):.3e}'
        description += f", MSE: {np.mean(loss_log['mse_loss']):.3e}"
        description += f", Emo: {np.mean(loss_log['emo_loss']):.3e}"
        description += f", Level: {np.mean(loss_log['level_loss']):.3e}"
        description += ']'
        logging.info(description)

        # Write to tensorboard
        if epoch % 50 == 0 and writer is not None:
            writer.add_scalar('train/total_loss', np.mean(loss_log['total_loss']), epoch)
            writer.add_scalar('train/mse_loss', np.mean(loss_log['mse_loss']), epoch)
            writer.add_scalar('train/emo_loss', np.mean(loss_log['emo_loss']), epoch)
            writer.add_scalar('train/level_loss', np.mean(loss_log['level_loss']), epoch)
            writer.add_scalar('opt/lr', optimizer.param_groups[0]['lr'], epoch)
            
            # Clear the loss log for next interval
            for key in loss_log.keys():
                loss_log[key].clear()

        # update learning rate  æ›´æ–°å­¦ä¹ ç‡
        if scheduler is not None and epoch < warm_iter + decay_iter:   # è°ƒåº¦å™¨ç”¨äºæ›´æ–°å­¦ä¹ ç‡  åŒºåˆ†ï¼šä¼˜åŒ–å™¨optimizor
            scheduler.step()

        # æ‰“å°è®­ç»ƒä¿¡æ¯
        if (epoch + 1) % 50 == 0:
            print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.item():.6f}")

    print("è®­ç»ƒå®Œæˆï¼")
    save_dir = f"{log_dir}/ckpt.pth"
    torch.save(model.state_dict(), save_dir)

def infer(emo_le = 2):
    tyro.extras.set_accent_color("bright_cyan")
    args = tyro.cli(ArgumentConfig)
    inf_cfg = partial_fields(InferenceConfig, args.__dict__)    # ä»args.__dict__é€‰å–InferenceConfigå¯¹åº”çš„å­—æ®µ ç»„æˆçš„å­—å…¸
    adef_wrapper = ADEFWrapper(inference_cfg=inf_cfg)       # æ ¸å¿ƒåŠŸèƒ½åŒ…è£…å™¨
    device = adef_wrapper.device          # cuda:0

    transf_model = EmotionTransformer().to(device)
    enhancer_p = '/mnt/disk2/zhouxishi/ADEF/pretrained_weights/ADEF/emo_enhancer/emo_enhancer.pth'
    transf_model_data = torch.load(enhancer_p, map_location=device)
    transf_model.load_state_dict(transf_model_data, strict=False)   # ['model']
    transf_model.eval()
    templates_dict = pickle.load(open('/mnt/disk2/zhouxishi/JoyVASA/src/my_prepare/joyvasa_motion_template.pkl', 'rb'))

    args.output_dir = f'/mnt/disk2/zhouxishi/JoyVASA/æµ‹è¯•æ•ˆæœ/eval/level{emo_le+1}'
    emo_level = torch.tensor(emo_le, dtype=torch.long).unsqueeze(0).to(device)
    image = '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/first_frame/M003_front_happy_level_3_027.png'
    audio = '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/videos/M003/front/happy/level_3/M003_front_happy_level_3_027.wav'
    test_datas = [(image, audio)]

    image_list = ['/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/first_frame/M003_front_angry_level_1_001.png',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/first_frame/M003_front_contempt_level_1_001.png',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/first_frame/M003_front_disgusted_level_1_001.png',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/first_frame/M003_front_fear_level_1_001.png',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/first_frame/M003_front_happy_level_1_001.png',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/first_frame/M003_front_neutral_level_1_001.png',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/first_frame/M003_front_sad_level_1_001.png',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/first_frame/M003_front_surprised_level_1_001.png',]

    audio_list = ['/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/videos/M003/front/angry/level_3/M003_front_angry_level_3_001.wav',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/videos/M003/front/contempt/level_3/M003_front_contempt_level_3_001.wav',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/videos/M003/front/disgusted/level_3/M003_front_disgusted_level_3_001.wav',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/videos/M003/front/fear/level_3/M003_front_fear_level_3_001.wav',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/videos/M003/front/happy/level_3/M003_front_happy_level_3_001.wav',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/videos/M003/front/neutral/level_1/M003_front_neutral_level_1_001.wav',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/videos/M003/front/sad/level_3/M003_front_sad_level_3_001.wav',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/videos/M003/front/surprised/level_3/M003_front_surprised_level_3_001.wav',]
    # test_datas = [(image_list[i],audio_list[i]) for i in range(len(audio_list))]

    for image,audio in test_datas:
    # for audio in audio_list:
        args.audio = audio
    ######## load reference image  åŠ è½½å‚è€ƒå›¾åƒ########
        img_rgb = load_image_rgb(image)       # (h,w,3)   (336,336,3)
        # è°ƒæ•´å›¾åƒçš„å¤§å°ï¼Œä½¿æœ€å¤§å°ºå¯¸ä¸è¶…è¿‡max_dimï¼Œå›¾åƒçš„å®½åº¦å’Œé«˜åº¦æ˜¯nï¼ˆdivisionï¼‰çš„å€æ•°ã€‚
        img_rgb = resize_to_limit(img_rgb, inf_cfg.source_max_dim, inf_cfg.source_division)       # (h,w,3)   (336,336,3)
        log(f"Load reference image from {image}")
        source_rgb_lst = [img_rgb]    # æºrgbå›¾åƒ åˆ—è¡¨ï¼Œsource_rgb_lst[0]å°±æ˜¯img_rgb    len==1
        
    #########  åŸæœ¬å®ç° #########################
        driving_template_dct = adef_wrapper.gen_motion_sequence(args)
        n_frames = driving_template_dct['n_frames']                        # ï¼ˆéŸ³é¢‘å¯¹åº”çš„ï¼‰æ€»å¸§æ•°

        emo_list = ['angry', 'contempt', 'disgusted', 'fear', 'happy', 'neutral', 'sad', 'surprised']
        emotype = audio.split('/')[-3]
        emo_id = int(emo_list.index(emotype))
        print(emo_id)
        # template_dict = templates_dict[emo_id]
        template_dict = templates_dict
        batch, least = n_frames//100, n_frames % 100        # å…ˆå†™æ­»100å¸§

        ori_exp = []
        for i in range(n_frames):
            exp = driving_template_dct['motion'][i]['exp']   # (1,21,3)
            exp = (exp.flatten() - template_dict["mean_exp"]) / (template_dict["std_exp"] + 1e-9)   # (63)
            exp = torch.tensor(exp, dtype=torch.float32).to(device)  # (63)
            ori_exp.append(exp)
        if least > 0:
            batch += 1
            addi = 100 - least
            for i in range(addi):
                ori_exp.append(ori_exp[-1])
        ori_exp =  torch.stack(ori_exp, dim=0).to(device)      # (batch * 100, 63)
        
        ## (n,1,63)
        # dit_cur = ori_exp[0:100].unsqueeze(0).to(device)       # (1, 100, 63)  
        # emo_index = torch.tensor([emo_id], dtype=torch.long).to(device)     # (1)
        # pred_delta_exp = transf_model(dit_cur,emo_index)    # (1, 1, 63)
        # pred_delta_exp = pred_delta_exp.cpu().detach() * template_dict["std_exp"] + template_dict["mean_exp"]    # [63]
        # pred_delta_exp = pred_delta_exp.reshape(21, 3).unsqueeze(0)                 #   (1,21,3)

        ## (n,100,63)
        res_exp = []
        for i in range(batch):
            dit_cur = ori_exp[i*100:(i+1)*100].unsqueeze(0).to(device)       # (1, 100, 63)  
            emo_index = torch.tensor([emo_id], dtype=torch.long).to(device)     # (1)
            pred_delta_exp = transf_model(dit_cur,emo_index,emo_level)    # (1, 1, 63)
            res_exp.append(pred_delta_exp)          # (1, 100, 63)
        res_exp = torch.stack(res_exp, dim=1).squeeze()     # (1, batch*100, 63) ->  (batch*100, 63)       # (b,25,63) -  (b*25,63)  -  (b*25,21,3) -  (batch*25, 1, 21, 3)
        exp_all = torch.zeros(1,21,3)
        for i in range(n_frames):
            # åå½’ä¸€åŒ–
            exp =  res_exp[i].cpu().detach() * template_dict["std_exp"] + template_dict["mean_exp"]    # [63]
            exp = exp.reshape(21, 3).unsqueeze(0)                 #   (1,21,3)
            exp_all += exp
        exp_all = exp_all / n_frames

        for i in range(n_frames):
            # driving_template_dct['motion'][i]['exp'] += 1 * pred_delta_exp.numpy() #  (1, 21, 3)
            driving_template_dct['motion'][i]['exp'] += 1 * exp_all.numpy()  #  (1, 21, 3)
        
        I_p_lst = []                                      # ç”Ÿæˆçš„è§†é¢‘çš„å›¾åƒåˆ—è¡¨ï¼Œè¿˜æ²¡æœ‰ç²˜è´´åˆ°åŸå›¾ç©ºé—´
        R_d_0, x_d_0_info = None, None             # ç¬¬ä¸€å¸§çš„æ—‹è½¬çŸ©é˜µï¼ˆR_d_0ï¼‰å’Œç›¸åº”çš„è¿åŠ¨ä¿¡æ¯ï¼ˆx_d_0_infoï¼‰ ï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿ

    ######## process source info å¤„ç†æºå‚è€ƒå›¾åƒç›¸å…³çš„ä¿¡æ¯ ########
        img_crop_256x256 = cv2.resize(source_rgb_lst[0], (256, 256))  # force to resize to 256x256  å¼ºåˆ¶è°ƒæ•´å¤§å°ä¸º256x256   ï¼ˆ256,256,3ï¼‰
        
        I_s = adef_wrapper.prepare_source(img_crop_256x256)         # å¤„ç†åçš„å›¾åƒ  H x W x 3  ->  B x 3 x H x W
        x_s_info = adef_wrapper.get_kp_info(I_s)                    # å‚è€ƒå›¾åƒçš„è®¡ç®—éšå¼å…³é”®ç‚¹æ—¶ç›¸å…³çš„ä¿¡æ¯
        x_c_s = x_s_info['kp']                                                    # æºå›¾åƒçš„è§„èŒƒå…³é”®ç‚¹Xc,s      B x 21 x 3 
        R_s = get_rotation_matrix(x_s_info['pitch'], x_s_info['yaw'], x_s_info['roll'])    # æ—‹è½¬çŸ©é˜µR         (B, 3, 3)
        f_s = adef_wrapper.extract_feature_3d(I_s)                  # å‚è€ƒå›¾åƒçš„å¤–è§‚ç‰¹å¾ï¼ˆé¢„è®­ç»ƒçš„Fï¼‰   [1, 32, 16, 64, 64]
        x_s = adef_wrapper.transform_keypoint(x_s_info)             # è¨ˆç®—æºéšå¼å…³é”®ç‚¹Xs,k       (bs, k, 3)  B x 21 x 3 

    ######## animate åŠ¨ç”»åŒ–ï¼ˆé€å¸§å¤„ç†éŸ³é¢‘ï¼‰########
        for i in track(range(n_frames), description='ğŸš€Animating Image with Generated Motions...', total=n_frames):
            x_d_i_info = driving_template_dct['motion'][i]   # åŒ…æ‹¬è¯¥å¸§çš„exp, scale, R, t, pitch, yaw, roll
            x_d_i_info = dct2device(x_d_i_info, device)
            
            # R  æ—‹è½¬çŸ©é˜µ
            R_d_i = x_d_i_info['R'] if 'R' in x_d_i_info.keys() else x_d_i_info['R_d']  # compatible with previous keys ä¸ä»¥å‰çš„é”®å…¼å®¹
            if i == 0:  # cache the first frame     ç¼“å­˜ç¬¬ä¸€å¸§
                R_d_0 = R_d_i                      # ç¬¬ä¸€å¸§çš„æ—‹è½¬çŸ©é˜µ
                x_d_0_info = x_d_i_info.copy()     # ç¬¬ä¸€å¸§çš„exp, scale, R, t, pitch, yaw, rollä¿¡æ¯
            if inf_cfg.animation_region == "all" or inf_cfg.animation_region == "pose":       # "exp", "pose", "lip", "eyes", "all"
                R_new = (R_d_i @ R_d_0.permute(0, 2, 1)) @ R_s  # Rd_i * Rd_0 * Rs    (B=1, 3, 3)
            else:        # "exp", "lip", "eyes"
                R_new = R_s           # ç›´æ¥copyå‚è€ƒå›¾åƒçš„æ—‹è½¬çŸ©é˜µ (B=1, 3, 3)

            # delta  è¡¨æƒ…å˜åŒ– ï¼ˆå¤§é“è‡³ç®€ï¼‰
            delta_new = x_d_i_info['exp']

            # scale  ç¼©æ”¾
            scale_new = x_s_info['scale'] * (x_d_i_info['scale'] / x_d_0_info['scale'])   # ï¼ˆ1,1ï¼‰

            # translation  å¤´éƒ¨å¹³ç§»
            t_new = x_s_info['t'] + (x_d_i_info['t'] - x_d_0_info['t'])    # åŸå›¾å¹³ç§»+ç›¸å¯¹é¦–å¸§å¹³ç§»    ï¼ˆ1,3ï¼‰
            t_new[..., 2].fill_(0)  # zero tz        zåæ ‡å˜ä¸º0

            x_d_i_new = scale_new * (x_c_s @ R_new + delta_new) + t_new         # è®¡ç®—è¯¥å¸§çš„éšå¼å…³é”®ç‚¹   ï¼ˆ1,21,3ï¼‰

            # æ‰­æ›²è§£ç ï¼ˆå›¾åƒç”Ÿæˆï¼‰ï¼šå¤–è§‚ç‰¹å¾ï¼Œæºéšå¼å…³é”®ç‚¹ï¼Œå½“å‰å¸§é©±åŠ¨éšå¼å…³é”®ç‚¹
            out = adef_wrapper.warp_decode(f_s, x_s, x_d_i_new)    
            I_p_i = adef_wrapper.parse_output(out['out'])[0]  # 512x512x3, uint8   ç”Ÿæˆçš„å›¾åƒï¼ˆå¯æ˜¾ç¤ºçš„é‚£ç§0~255ï¼‰
            I_p_lst.append(I_p_i)         # å›¾åƒåˆ—è¡¨ï¼ˆè¿˜æ²¡ç²˜è´´å›å®Œæ•´åŸå›¾ç©ºé—´ï¼‰

        # save the animated result       ä¿å­˜ç»“æœ 
        mkdir(args.output_dir)
        temp_video = osp.join(args.output_dir, f'{basename(image)}_{basename(audio)}_temp.mp4')
        images2video(I_p_lst, wfp=temp_video, fps=inf_cfg.output_fps)
        final_video = osp.join(args.output_dir, f'{basename(image)}_{basename(audio)}.mp4')
        add_audio_to_video(temp_video, audio, final_video)     # æ·»åŠ éŸ³è½¨
    return None

if __name__ == "__main__":
    # transf_train()
    infer()
