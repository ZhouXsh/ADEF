# coding: utf-8

"""
Pipeline of LivePortrait with Audio-Driven Motion Generation (Human)
ï¼ï¼ï¼
"""

import numpy as np
from src.modules.audio2emotion import AudioEmotionClassifierModel
import torch

torch.backends.cudnn.benchmark = True # disable CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR warning

import cv2; cv2.setNumThreads(0); cv2.ocl.setUseOpenCL(False)
import os
import os.path as osp
 
from rich.progress import track

from .config.argument_config import ArgumentConfig
from .config.inference_config import InferenceConfig
from .config.crop_config import CropConfig
from .utils.cropper import Cropper
from .utils.camera import get_rotation_matrix
from .utils.video import images2video, add_audio_to_video
from .utils.crop import prepare_paste_back, paste_back
from .utils.io import load_image_rgb, resize_to_limit, load, dump
from .utils.helper import mkdir, basename, dct2device, is_image, calc_motion_multiplier, is_template, remove_suffix
from .utils.rprint import rlog as log
from .utils.viz import viz_lmk, plot_3d_scatter, plot_vectors, plot_vector_pairs
from .ADEF_wrapper import ADEFWrapper         # ç‹¬ç«‹åˆ†å¸ƒ+æƒ…æ„Ÿå½’ä¸€åŒ–

############       è¿™ä¸ªå†™åœ¨configé‡Œé¢åº”è¯¥æ›´å¥½
emo_list = ['angry', 'contempt', 'disgusted', 'fear', 'happy', 'neutral', 'sad', 'surprised']
emo_label = ['ang',  'con',  'dis',  'fea',  'hap',  'neu',  'sad',  'sur']

def make_abs_path(fn):      # ç”Ÿæˆç»å¯¹è·¯å¾„
    return osp.join(osp.dirname(osp.realpath(__file__)), fn)

import numpy as np
from scipy.optimize import curve_fit

def smooth_driving_template_cosine(left_frame, right_frame, driving_template_dct):
    """
    å¯¹æŒ‡å®šå¸§åŒºé—´å†…çš„ driving_template_dct è¿›è¡Œä½™å¼¦æ’å€¼å’Œå¹³æ»‘æ‹Ÿåˆã€‚

    å‚æ•°:
        left_frame (int): åŒºé—´èµ·å§‹å¸§ç´¢å¼•
        right_frame (int): åŒºé—´ç»“æŸå¸§ç´¢å¼•
        driving_template_dct (dict): å­˜å‚¨å…³é”®å¸§å‚æ•°çš„å­—å…¸ï¼Œæ¯ä¸ªé”®ä¸ºå¸§ç¼–å·

    ä¿®æ”¹:
        è¯¥å‡½æ•°å°†ç›´æ¥ä¿®æ”¹ driving_template_dct ä¸­ left_frame åˆ° right_frame åŒºé—´å†…çš„æ•°æ®ã€‚
    """

    # ------------------ scale, t, R è¿›è¡Œä½™å¼¦æ’å€¼ ------------------
    for i in range(left_frame, right_frame):
        t = (i - left_frame) / (right_frame - left_frame)
        cos_val = np.cos(np.pi * t)  # t âˆˆ [0,1] ä¸Šçš„ä½™å¼¦å€¼

        for key in ['scale', 't', 'R']:
            left_val = driving_template_dct[left_frame][key]
            right_val = driving_template_dct[right_frame][key]
            A = 0.5 * (left_val - right_val)
            C = 0.5 * (left_val + right_val)
            driving_template_dct[i][key] = A * cos_val + C

    # ------------------ è¡¨è¾¾å‚æ•° exp çš„ä½™å¼¦æ‹Ÿåˆ ------------------

    def cosine_func(x, A, B, C, D):
        return A * np.cos(B * x + C) + D

    def estimate_initial_params(x, y):
        D = np.mean(y)
        A = (np.max(y) - np.min(y)) / 2
        T = x[-1] - x[0]
        B = 2 * np.pi / T if T != 0 else 1.0
        return [A, B, 0, D]

    x_data = np.arange(left_frame, right_frame + 1)
    y_all = np.array([
        driving_template_dct[i]['exp'].reshape(-1)
        for i in x_data
    ])  # shape: (N, 63)

    y_fit = np.zeros_like(y_all)

    for kp in range(63):
        y_kp = y_all[:, kp]
        try:
            p0 = estimate_initial_params(x_data, y_kp)
            params, _ = curve_fit(cosine_func, x_data, y_kp, p0=p0, maxfev=10000)
            y_fit[:, kp] = cosine_func(x_data, *params)
        except RuntimeError:
            y_fit[:, kp] = y_kp  # æ‹Ÿåˆå¤±è´¥åˆ™ä¿ç•™åŸå§‹å€¼

    for i, frame_id in enumerate(x_data):
        driving_template_dct[frame_id]['exp'] = y_fit[i].reshape(1, 21, 3)


class LivePortraitPipeline(object):
    def __init__(self, inference_cfg: InferenceConfig, crop_cfg: CropConfig):
        self.adef_wrapper: ADEFWrapper = ADEFWrapper(inference_cfg=inference_cfg)       # æ ¸å¿ƒåŠŸèƒ½åŒ…è£…å™¨
        self.cropper: Cropper = Cropper(crop_cfg=crop_cfg)                        # è£å‰ªå™¨
    
    def execute(self, args: ArgumentConfig, ):
        inf_cfg = self.adef_wrapper.inference_cfg          # æ¨ç†é…ç½®
        device = self.adef_wrapper.device          # cuda:0
        crop_cfg = self.cropper.crop_cfg                           # è£å‰ªå™¨å‚æ•°

######## load reference image  åŠ è½½å‚è€ƒå›¾åƒ########
        if is_image(args.reference):    # å‚è€ƒå›¾åƒ
            img_rgb = load_image_rgb(args.reference)       # (h,w,3)   (336,336,3)
            # è°ƒæ•´å›¾åƒçš„å¤§å°ï¼Œä½¿æœ€å¤§å°ºå¯¸ä¸è¶…è¿‡max_dimï¼Œå›¾åƒçš„å®½åº¦å’Œé«˜åº¦æ˜¯nï¼ˆdivisionï¼‰çš„å€æ•°ã€‚
            img_rgb = resize_to_limit(img_rgb, inf_cfg.source_max_dim, inf_cfg.source_division)       # (h,w,3)   (336,336,3)
            log(f"Load reference image from {args.reference}")
            source_rgb_lst = [img_rgb]    # æºrgbå›¾åƒ åˆ—è¡¨ï¼Œsource_rgb_lst[0]å°±æ˜¯img_rgb    len==1
        else:
            raise Exception(f"Unknown reference image format: {args.reference}")
        
        if inf_cfg.use_emo_analyzer:     # æƒ…æ„Ÿåˆ†æ
            np_path = args.audio[:-4] + '.npy'
            if not os.path.exists(np_path):      # ä»npyæ¨¡ç‰ˆåŠ è½½
                from funasr import AutoModel
                model = AutoModel(
                    model="iic/emotion2vec_plus_large",
                    hub="ms",  # "ms" or "modelscope" for China mainland users; "hf" or "huggingface" for other overseas users
                )
                model.generate(args.audio, output_dir=os.path.dirname(args.audio), granularity="utterance", extract_embedding=True)
            emo_vec = torch.tensor(np.load(np_path)).unsqueeze(0).to(device)  # [1, 1024]

            a2e_model = AudioEmotionClassifierModel().to(device)
            dict = torch.load('/mnt/disk2/zhouxishi/ADEF/pretrained_weights/ADEF/audio2emo/audio2emo.pth',map_location=device)
            a2e_model.load_state_dict(dict)
            a2e_model.eval()

            outputs = a2e_model(emo_vec)
            argmax = outputs.argmax(dim=1).item()
            args.emotype = emo_list[argmax]
            print(f"Emotion: {args.emotype}")

# ####### æˆ‘å†™çš„ ######## ä»æ¨¡ç‰ˆä¸­è¯»å–motion  ï¼ˆç”¨äºéªŒè¯ï¼ŒåŠ é€Ÿé©±åŠ¨ï¼‰        ##########################
#         wfp_template = remove_suffix(args.audio) + '.pkl'     # ä¿å­˜åˆ°æŒ‡å®šç›®å½•
#         if os.path.exists(wfp_template):      # ä»pklæ¨¡ç‰ˆåŠ è½½
#             log(f"Load from template: {wfp_template}.", style='bold green')
#             driving_template_dct = load(wfp_template)
#         else:
# ######## generate motion sequence æ ¹æ®éŸ³é¢‘ç”Ÿæˆè¿åŠ¨åºåˆ—   ï¼ˆåˆ›æ–°ä¹‹å¤„ï¼‰########
#             driving_template_dct = self.adef_wrapper.gen_motion_sequence(args)   # {'n_frames':XX, 'output_fps':XX, 'motions':[{exp,t,...},{},{},...,{}å…±n_framesä¸ªå­—å…¸]}
#             dump(wfp_template, driving_template_dct)
#             log(f"Dump motion template to {wfp_template}")

#########  åŸæœ¬å®ç° #########################
        driving_template_dct = self.adef_wrapper.gen_motion_sequence(args)
        n_frames = driving_template_dct['n_frames']                        # ï¼ˆéŸ³é¢‘å¯¹åº”çš„ï¼‰æ€»å¸§æ•°
        print(n_frames)

        if args.use_emo_enhancer:     # æƒ…æ„Ÿå¢å¼º
            for window in range(99,n_frames,100):   # å¹³æ»‘çª—å£
                left_frame = window - 10
                right_frame = window + 10
                if left_frame < 0 or right_frame >= n_frames:
                    continue
                smooth_driving_template_cosine(left_frame, right_frame, driving_template_dct['motion'])

######## prepare for pasteback å°†è¢«è£å‰ªçš„å›¾åƒç²˜è´´å›åŸå›¾åƒä½ç½® ########
        I_p_pstbk_lst = None                              # ç”Ÿæˆçš„è§†é¢‘çš„å›¾åƒåˆ—è¡¨ï¼Œå¹¶ä¸”ç²˜è´´å›å®Œæ•´çš„åŸå›¾ç©ºé—´
        if inf_cfg.flag_pasteback and inf_cfg.flag_do_crop and inf_cfg.flag_stitching:   # éœ€è¦ç²˜è´´å›ï¼›éœ€è¦è¢«è£å‰ªï¼›éœ€è¦è¢«ç¼åˆ
            I_p_pstbk_lst = []                            # ç”¨äºå­˜å‚¨ç»è¿‡ pasteback å¤„ç†åçš„å›¾åƒå¸§
            log("Prepared pasteback mask done.")
        I_p_lst = []                                      # ç”Ÿæˆçš„è§†é¢‘çš„å›¾åƒåˆ—è¡¨ï¼Œè¿˜æ²¡æœ‰ç²˜è´´åˆ°åŸå›¾ç©ºé—´
        R_d_0, x_d_0_info = None, None             # ç¬¬ä¸€å¸§çš„æ—‹è½¬çŸ©é˜µï¼ˆR_d_0ï¼‰å’Œç›¸åº”çš„è¿åŠ¨ä¿¡æ¯ï¼ˆx_d_0_infoï¼‰ ï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿ
        flag_normalize_lip = inf_cfg.flag_normalize_lip  # not overwrite          False                      # è¿›è¡Œå˜´å”‡çš„æ ‡å‡†åŒ–å¤„ç†ï¼ˆFalseï¼‰
        flag_source_video_eye_retargeting = inf_cfg.flag_source_video_eye_retargeting  # not overwrite       # æ˜¯å¦å¯ç”¨åŸè§†é¢‘çœ¼ç›é‡å®šå‘ï¼ˆFalseï¼‰
        lip_delta_before_animation, eye_delta_before_animation = None, None      # åŠ¨ç”»å‰çš„å˜´å”‡å’Œçœ¼ç›ä½ç§»å·®å¼‚

######## process source info å¤„ç†æºå‚è€ƒå›¾åƒç›¸å…³çš„ä¿¡æ¯ ########
        if inf_cfg.flag_do_crop:     # å‚è€ƒå›¾åƒéœ€è¦è¢«è£å‰ª
            crop_info = self.cropper.crop_source_image(source_rgb_lst[0], crop_cfg)     # è¿”å›landmarkç­‰ä¿¡æ¯
            if crop_info is None:
                raise Exception("No face detected in the source image!")
            source_lmk = crop_info['lmk_crop']                             # å›¾åƒçš„landmark
            img_crop_256x256 = crop_info['img_crop_256x256']               # è£å‰ªæˆ256x256åçš„å›¾åƒ
        else:           # å‚è€ƒå›¾åƒæœ¬å°±æ˜¯è£å‰ªè¿‡çš„
            source_lmk = self.cropper.calc_lmk_from_cropped_image(source_rgb_lst[0])    # å›¾åƒçš„landmark   # (203, 2)
            img_crop_256x256 = cv2.resize(source_rgb_lst[0], (256, 256))  # force to resize to 256x256  å¼ºåˆ¶è°ƒæ•´å¤§å°ä¸º256x256   ï¼ˆ256,256,3ï¼‰
        I_s = self.adef_wrapper.prepare_source(img_crop_256x256)         # å¤„ç†åçš„å›¾åƒ  H x W x 3  ->  B x 3 x H x W
        x_s_info = self.adef_wrapper.get_kp_info(I_s)                    # å‚è€ƒå›¾åƒçš„è®¡ç®—éšå¼å…³é”®ç‚¹æ—¶ç›¸å…³çš„ä¿¡æ¯
            #     x_s_info = kp_info = out = ret_dct = {    ä»Motion Extractorå¾—åˆ°
            #         'pitch': pitch,        # (B, C=1)
            #         'yaw': yaw,            # (B, C=1)
            #         'roll': roll,          # (B, C=1)
            #         't': t,                # (B, C=3)
            #         'exp': exp,            # (B, num_kp = 21, C=3)
            #         'scale': scale,        # (B, C=1)
            #         'kp': kp,   # canonical keypoint   (B, num_kp = 21, C=3)
            #     }
        x_c_s = x_s_info['kp']                                                    # æºå›¾åƒçš„è§„èŒƒå…³é”®ç‚¹Xc,s      B x 21 x 3 
        R_s = get_rotation_matrix(x_s_info['pitch'], x_s_info['yaw'], x_s_info['roll'])    # æ—‹è½¬çŸ©é˜µR         (B, 3, 3)
        f_s = self.adef_wrapper.extract_feature_3d(I_s)                  # å‚è€ƒå›¾åƒçš„å¤–è§‚ç‰¹å¾ï¼ˆé¢„è®­ç»ƒçš„Fï¼‰   [1, 32, 16, 64, 64]
        
        x_s = self.adef_wrapper.transform_keypoint(x_s_info)             # è¨ˆç®—æºéšå¼å…³é”®ç‚¹Xs,k       (bs, k, 3)  B x 21 x 3 

        # let lip-open scalar to be 0 at first  é¦–å…ˆè®©å”‡å¼ å¼€æ ‡é‡ä¸º0
        if flag_normalize_lip and inf_cfg.flag_relative_motion and source_lmk is not None:               # âˆš
            c_d_lip_before_animation = [0.]       # å”‡éƒ¨å¼ å¼€åº¦ï¼Œåˆå§‹å€¼è®¾ç½®ä¸ºé›¶
            combined_lip_ratio_tensor_before_animation = self.adef_wrapper.calc_combined_lip_ratio(c_d_lip_before_animation, source_lmk)   # è®¡ç®—æœ€ç»ˆçš„å”‡éƒ¨å¼ å¼€åº¦æ¯”ä¾‹
            if combined_lip_ratio_tensor_before_animation[0][0] >= inf_cfg.lip_normalize_threshold:  # å”‡éƒ¨å¼ å¼€åº¦å¤§äºç­‰äºæŸä¸ªé˜ˆå€¼ï¼šè¿›è¡Œåç»­çš„å”‡éƒ¨è°ƒæ•´æ“ä½œ
                lip_delta_before_animation = self.adef_wrapper.retarget_lip(x_s, combined_lip_ratio_tensor_before_animation)   # é‡å®šå‘å˜´å”‡

        if inf_cfg.flag_pasteback and inf_cfg.flag_do_crop and inf_cfg.flag_stitching:         # éœ€è¦ç²˜è´´å›ï¼›éœ€è¦è¢«è£å‰ªï¼›éœ€è¦è¢«ç¼åˆ  False
            mask_ori_float = prepare_paste_back(inf_cfg.mask_crop, crop_info['M_c2o'], dsize=(source_rgb_lst[0].shape[1], source_rgb_lst[0].shape[0]))

######## animate åŠ¨ç”»åŒ–ï¼ˆé€å¸§å¤„ç†éŸ³é¢‘ï¼‰########
        for i in track(range(n_frames), description='ğŸš€Animating Image with Generated Motions...', total=n_frames):

            x_d_i_info = driving_template_dct['motion'][i]   # åŒ…æ‹¬è¯¥å¸§çš„exp, scale, R, t, pitch, yaw, roll
            x_d_i_info = dct2device(x_d_i_info, device)
            
            # R  æ—‹è½¬çŸ©é˜µ
            R_d_i = x_d_i_info['R'] if 'R' in x_d_i_info.keys() else x_d_i_info['R_d']  # compatible with previous keys ä¸ä»¥å‰çš„é”®å…¼å®¹
            if i == 0:  # cache the first frame     ç¼“å­˜ç¬¬ä¸€å¸§
                R_d_0 = R_d_i                      # ç¬¬ä¸€å¸§çš„æ—‹è½¬çŸ©é˜µ
                x_d_0_info = x_d_i_info.copy()     # ç¬¬ä¸€å¸§çš„exp, scale, R, t, pitch, yaw, rollä¿¡æ¯
            if inf_cfg.animation_region == "all" or inf_cfg.animation_region == "pose":       # "exp", "pose", "lip", "eyes", "all"
                R_new = (R_d_i @ R_d_0.permute(0, 2, 1)) @ R_s  # Rd_i * Rd_0 * Rs    (B=1, 3, 3)
            else:        # "exp", "lip", "eyes"
                R_new = R_s           # ç›´æ¥copyå‚è€ƒå›¾åƒçš„æ—‹è½¬çŸ©é˜µ (B=1, 3, 3)

            delta_new = x_s_info['exp'].clone()   # ï¼ˆ1,21,3ï¼‰
            if inf_cfg.animation_region == "all" or inf_cfg.animation_region == "exp":  # "exp", "pose", "lip", "eyes", "all"
                # ç›¸å¯¹
                delta_new = x_s_info['exp'] + (x_d_i_info['exp'] - x_d_0_info['exp'])   # åŸå›¾å˜åŒ–+ç›¸å¯¹é¦–å¸§å˜åŒ–
                for lip_idx in [6, 12, 14, 17, 19, 20]:
                    delta_new[:, lip_idx, :] = x_d_i_info['exp'][:, lip_idx, :]
                # # ç»å¯¹
                # delta_new = x_s_info['exp'].clone()
                # for idx in [1,2,6,11,12,13,14,15,16,17,18,19,20]:
                #     delta_new[:, idx, :] = x_d_i_info['exp'][:, idx, :]
                # delta_new[:, 3:5, 1] =  x_d_i_info['exp'][:, 3:5, 1]
                # delta_new[:, 5, 2] =  x_d_i_info['exp'][:, 5, 2]
                # delta_new[:, 8, 2] =  x_d_i_info['exp'][:, 8, 2]
                # delta_new[:, 9, 1:] = x_d_i_info['exp'][:, 9, 1:]
            elif inf_cfg.animation_region == "lip":
                for lip_idx in [6, 12, 14, 17, 19, 20]:
                    delta_new[:, lip_idx, :] = (x_s_info['exp'] + (x_d_i_info['exp'] - x_d_0_info['exp']))[:, lip_idx, :]
            
            # scale  ç¼©æ”¾
            scale_new = x_s_info['scale'] * (x_d_i_info['scale'] / x_d_0_info['scale'])   # ï¼ˆ1,1ï¼‰

            # translation  å¤´éƒ¨å¹³ç§»
            t_new = x_s_info['t'] + (x_d_i_info['t'] - x_d_0_info['t'])    # åŸå›¾å¹³ç§»+ç›¸å¯¹é¦–å¸§å¹³ç§»    ï¼ˆ1,3ï¼‰
            # t_new = x_s_info['t']
            t_new[..., 2].fill_(0)  # zero tz        zåæ ‡å˜ä¸º0

            x_d_i_new = scale_new * (x_c_s @ R_new + delta_new) + t_new         # è®¡ç®—è¯¥å¸§çš„éšå¼å…³é”®ç‚¹   ï¼ˆ1,21,3ï¼‰

            if inf_cfg.flag_relative_motion and inf_cfg.driving_option == "expression-friendly":   # ç›¸å¯¹è¿åŠ¨ & è¡¨æƒ…å‹å¥½å‹çš„
                if i == 0:
                    x_d_0_new = x_d_i_new          # ä¿ç•™ç¬¬ä¸€å¸§ï¼ˆç¬¬ä¸€å¸§ä¸éœ€è¦å€å¢ï¼‰
                    motion_multiplier = calc_motion_multiplier(x_s, x_d_0_new)    # åŸºäºæºå›¾åƒå’Œç¬¬ä¸€é©±åŠ¨å¸§è®¡ç®—è¿åŠ¨å€å¢å™¨
                x_d_diff = (x_d_i_new - x_d_0_new) * motion_multiplier     # ç›¸å¯¹ç¬¬ä¸€å¸§çš„å·®å¼‚ï¼ˆä¹˜ä»¥å€å¢å™¨ï¼‰
                x_d_i_new = x_d_diff + x_s

            # Algorithm 1 in Liveportrait:
            if not inf_cfg.flag_stitching and not inf_cfg.flag_eye_retargeting and not inf_cfg.flag_lip_retargeting:     # æ— ç¼åˆæ— é‡å®šå‘
                # without stitching or retargeting
                if flag_normalize_lip and lip_delta_before_animation is not None:
                    x_d_i_new += lip_delta_before_animation
                if flag_source_video_eye_retargeting and eye_delta_before_animation is not None:
                    x_d_i_new += eye_delta_before_animation
                else:
                    pass
            elif inf_cfg.flag_stitching and not inf_cfg.flag_eye_retargeting and not inf_cfg.flag_lip_retargeting:        # æœ‰ç¼åˆä½†æ— é‡å®šå‘
                # with stitching and without retargeting
                if flag_normalize_lip and lip_delta_before_animation is not None:
                    x_d_i_new = self.adef_wrapper.stitching(x_s, x_d_i_new) + lip_delta_before_animation
                else:
                    x_d_i_new = self.adef_wrapper.stitching(x_s, x_d_i_new)
                if flag_source_video_eye_retargeting and eye_delta_before_animation is not None:
                    x_d_i_new += eye_delta_before_animation
            else:
                if inf_cfg.flag_relative_motion:  # use x_s
                    x_d_i_new = x_s
                else:  # use x_d,i
                    x_d_i_new = x_d_i_new
                if inf_cfg.flag_stitching:
                    x_d_i_new = self.adef_wrapper.stitching(x_s, x_d_i_new)

            x_d_i_new = x_s + (x_d_i_new - x_s) * inf_cfg.driving_multiplier      # ï¼ˆ1,21,3ï¼‰
            # æ‰­æ›²è§£ç ï¼ˆå›¾åƒç”Ÿæˆï¼‰ï¼šå¤–è§‚ç‰¹å¾ï¼Œæºéšå¼å…³é”®ç‚¹ï¼Œå½“å‰å¸§é©±åŠ¨éšå¼å…³é”®ç‚¹
            out = self.adef_wrapper.warp_decode(f_s, x_s, x_d_i_new)    
            I_p_i = self.adef_wrapper.parse_output(out['out'])[0]  # 512x512x3, uint8   ç”Ÿæˆçš„å›¾åƒï¼ˆå¯æ˜¾ç¤ºçš„é‚£ç§0~255ï¼‰
            I_p_lst.append(I_p_i)         # å›¾åƒåˆ—è¡¨ï¼ˆè¿˜æ²¡ç²˜è´´å›å®Œæ•´åŸå›¾ç©ºé—´ï¼‰

            if inf_cfg.flag_pasteback and inf_cfg.flag_do_crop and inf_cfg.flag_stitching:     # éœ€è¦ç²˜è´´å›ï¼›éœ€è¦è¢«è£å‰ªï¼›éœ€è¦è¢«ç¼åˆ
                # TODO: the paste back procedure is slow, considering optimize it using multi-threading or GPU  å›ç²˜è´´è¿‡ç¨‹å¾ˆæ…¢ï¼›è€ƒè™‘ä½¿ç”¨å¤šçº¿ç¨‹æˆ–GPUå¯¹å…¶è¿›è¡Œä¼˜åŒ–
                I_p_pstbk = paste_back(I_p_i, crop_info['M_c2o'], source_rgb_lst[0], mask_ori_float)    # å°†ç”Ÿæˆçš„å›¾åƒç²˜è´´å›åŸç©ºé—´å çš„å®Œæ•´å›¾åƒ
                I_p_pstbk_lst.append(I_p_pstbk)

        # save the animated result       ä¿å­˜ç»“æœ 
        mkdir(args.output_dir)
        temp_video = osp.join(args.output_dir, f'{basename(args.reference)}_{basename(args.audio)}_temp.mp4')
        # å°†å›¾ç‰‡åˆ—è¡¨ä¿å­˜ä¸ºè§†é¢‘ï¼ˆæ²¡å£°éŸ³ï¼‰
        if I_p_pstbk_lst is not None and len(I_p_pstbk_lst) > 0:
            print(f'å›¾åƒæ•°é‡ï¼ˆå¸§æ•°ï¼‰    {len(I_p_pstbk_lst)}')
            images2video(I_p_pstbk_lst, wfp=temp_video, fps=inf_cfg.output_fps)
        else:
            print(f'å›¾åƒæ•°é‡ï¼ˆå¸§æ•°ï¼‰    {len(I_p_lst)}')
            images2video(I_p_lst, wfp=temp_video, fps=inf_cfg.output_fps)
        final_video = osp.join(args.output_dir, f'{basename(args.reference)}_{basename(args.audio)}.mp4')
        add_audio_to_video(temp_video, args.audio, final_video)     # æ·»åŠ éŸ³è½¨
        return final_video