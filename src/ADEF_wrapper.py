# coding: utf-8

"""
ç‹¬ç«‹åˆ†å¸ƒ  çš„  æƒ…æ„ŸåŒ…è£…å™¨
"""

import contextlib
import os.path as osp
import os
import pickle
import numpy as np
import cv2
from src.modules.emotion_enhancer import EmotionTransformer
import torch
import yaml
import math
import librosa
import torch.nn.functional as F
from rich.progress import track

from .utils.timer import Timer
from .utils.helper import load_model, concat_feat
from .utils.camera import headpose_pred_to_degree, get_rotation_matrix
from .utils.retargeting_utils import calc_eye_close_ratio, calc_lip_close_ratio
from .config.inference_config import InferenceConfig
from .utils.rprint import rlog as log
from .utils.filter import smooth_

############       è¿™ä¸ªå†™åœ¨configé‡Œé¢åº”è¯¥æ›´å¥½
emo_list = ['angry', 'contempt', 'disgusted', 'fear', 'happy', 'neutral', 'sad', 'surprised']
emo_label = ['ang',  'con',  'dis',  'fea',  'hap',  'neu',  'sad',  'sur']

class ADEFWrapper(object):
    def __init__(self, inference_cfg: InferenceConfig):

        self.inference_cfg = inference_cfg
        self.device_id = inference_cfg.device_id
        self.compile = inference_cfg.flag_do_torch_compile
        if inference_cfg.flag_force_cpu:
            self.device = 'cpu'
        else:
            try:
                if torch.backends.mps.is_available():
                    self.device = 'mps'
                else:
                    self.device = 'cuda:' + str(self.device_id)
            except:
                self.device = 'cuda:' + str(self.device_id)
        
        model_config = yaml.load(open(inference_cfg.models_config, 'r'), Loader=yaml.SafeLoader)
        # init F   å¤–è§‚ç‰¹å¾æå–å™¨
        self.appearance_feature_extractor = load_model(inference_cfg.checkpoint_F, model_config, self.device, 'appearance_feature_extractor')
        log(f'Load appearance_feature_extractor from {osp.realpath(inference_cfg.checkpoint_F)} done.')
        # init M    è¿åŠ¨æå–å™¨
        self.motion_extractor = load_model(inference_cfg.checkpoint_M, model_config, self.device, 'motion_extractor')
        log(f'Load motion_extractor from {osp.realpath(inference_cfg.checkpoint_M)} done.')
        # init W    æ‰­æ›²æ¨¡å—
        self.warping_module = load_model(inference_cfg.checkpoint_W, model_config, self.device, 'warping_module')
        log(f'Load warping_module from {osp.realpath(inference_cfg.checkpoint_W)} done.')
        # init G      å›¾åƒç”Ÿæˆå™¨     
        self.spade_generator = load_model(inference_cfg.checkpoint_G, model_config, self.device, 'spade_generator')
        log(f'Load spade_generator from {osp.realpath(inference_cfg.checkpoint_G)} done.')
        # init S and R     ç¼åˆæ¨¡å— å’Œ é‡å®šå‘æ¨¡å—
        if inference_cfg.checkpoint_S is not None and osp.exists(inference_cfg.checkpoint_S):
            self.stitching_retargeting_module = load_model(inference_cfg.checkpoint_S, model_config, self.device, 'stitching_retargeting_module')
            log(f'Load stitching_retargeting_module from {osp.realpath(inference_cfg.checkpoint_S)} done.')
        else:
            self.stitching_retargeting_module = None
        # Optimize for inference
        if self.compile:
            torch._dynamo.config.suppress_errors = True  # Suppress errors and fall back to eager execution
            self.warping_module = torch.compile(self.warping_module, mode='max-autotune')
            self.spade_generator = torch.compile(self.spade_generator, mode='max-autotune')

        self.model_config = model_config
        self.timer = Timer()

        # Motion Genertor     è¿åŠ¨ç”Ÿæˆå™¨
        self.motion_generator, self.motion_generator_args = load_model(inference_cfg.checkpoint_MotionGenerator, model_config, self.device, 'motion_generator')
        log(f'Load motion_generator from {osp.realpath(inference_cfg.checkpoint_MotionGenerator)} done.')
        self.n_motions = self.motion_generator_args.n_motions                  # å•ä¸ªéŸ³é¢‘å­åºåˆ—ï¼ˆç‰‡æ®µï¼‰çš„å¸§æ•°
        self.n_prev_motions = self.motion_generator_args.n_prev_motions
        self.fps = self.motion_generator_args.fps
        self.audio_unit = 16000. / self.fps     # num of samples per frame     æ¯å¸§æ ·æœ¬æ•°
        self.n_audio_samples = round(self.audio_unit * self.n_motions)
        self.pad_mode = self.motion_generator_args.pad_mode
        self.use_indicator = self.motion_generator_args.use_indicator
        self.templete_dict = pickle.load(open(inference_cfg.motion_template_path, 'rb'))

        # #### 0420æƒ…æ„Ÿå¢å¼º
        self.emo_ehance = inference_cfg.use_emo_enhancer
        if self.emo_ehance:
            transf_model = EmotionTransformer().to(self.device)
            enhancer_p = '/mnt/disk2/zhouxishi/ADEF/pretrained_weights/ADEF/emo_enhancer/emo_enhancer.pth'
            transf_model_data = torch.load(enhancer_p, map_location=self.device)
            transf_model.load_state_dict(transf_model_data, strict=False)
            transf_model.eval()
            transf_model.to(self.device)
            self.emo_enhancer = transf_model
            log(f'load enhancer_p from {enhancer_p}')

    # è·å–æ¨ç†ä¸Šä¸‹æ–‡
    def inference_ctx(self):
        if self.device == "mps":
            ctx = contextlib.nullcontext()
        else:
            ctx = torch.autocast(device_type=self.device[:4], dtype=torch.float16,
                                 enabled=self.inference_cfg.flag_use_half_precision)
        return ctx

    # ï¼ˆæ ¹æ®ç”¨æˆ·ä¿®æ”¹åï¼‰æ›´æ–°é…ç½®
    def update_config(self, user_args):
        for k, v in user_args.items():
            if hasattr(self.inference_cfg, k):
                setattr(self.inference_cfg, k, v)

    # å¤„ç†æºå›¾åƒ    OK
    def prepare_source(self, img: np.ndarray) -> torch.Tensor:
        """ construct the input as standard å°†è¾“å…¥æ„å»ºä¸ºæ ‡å‡†
        img: H x W x 3, uint8ï¼ˆ0~255ï¼‰, 256x256         ï¼ˆ256,256,3ï¼‰
        è¾“å‡ºx: B x 3 x H x W, float32, 256x256        B=1
        """
        h, w = img.shape[:2]
        if h != self.inference_cfg.input_shape[0] or w != self.inference_cfg.input_shape[1]:        # 256,256
            x = cv2.resize(img, (self.inference_cfg.input_shape[0], self.inference_cfg.input_shape[1]))
        else:
            x = img.copy()

        if x.ndim == 3:   # ä½¿ç”¨np.newaxiså¢åŠ ç»´åº¦
            x = x[np.newaxis].astype(np.float32) / 255.  # H x W x 3 -> 1 x H x W x 3, å½’ä¸€åŒ–normalized to 0~1
        elif x.ndim == 4:        # å·²ç»æ˜¯ä¸€ä¸ªå›¾åƒæ‰¹æ¬¡äº†
            x = x.astype(np.float32) / 255.  # B x H x W x 3, å½’ä¸€åŒ–normalized to 0~1
        else:
            raise ValueError(f'img ndim should be 3 or 4: {x.ndim}')
        x = np.clip(x, 0, 1)  # clip to 0~1     å°†å›¾åƒçš„æ‰€æœ‰åƒç´ å€¼é™åˆ¶åœ¨ [0, 1] èŒƒå›´å†…ï¼Œé˜²æ­¢å‡ºç°å½’ä¸€åŒ–åçš„å€¼è¶…è¿‡ 1 æˆ–å°äº 0ã€‚
        x = torch.from_numpy(x).permute(0, 3, 1, 2)  # 1xHxWx3 -> 1x3xHxW
        x = x.to(self.device)
        return x     # 1 x 3 x H x W

    # å¤„ç†è§†é¢‘
    def prepare_videos(self, imgs) -> torch.Tensor:
        """ construct the input as standard å°†è¾“å…¥æ„å»ºä¸ºæ ‡å‡†Tensor
        imgs: N x B x H x W x 3, uint8
        """
        if isinstance(imgs, list):   # åˆ—è¡¨ç±»å‹
            _imgs = np.array(imgs)[..., np.newaxis]    # T x H x W x 3 -> T x H x W x 3 x 1
        elif isinstance(imgs, np.ndarray):
            _imgs = imgs                               # T x H x W x 3 x 1
        else:
            raise ValueError(f'imgs type error: {type(imgs)}')

        y = _imgs.astype(np.float32) / 255.       # å½’ä¸€åŒ–åˆ° [0,1]
        y = np.clip(y, 0, 1)  # clip to 0~1       # ç¡®ä¿æ•°æ®èŒƒå›´åœ¨ [0,1]ï¼Œé˜²æ­¢å¼‚å¸¸æ•°æ®ï¼ˆnp.clip é™åˆ¶æœ€å¤§å€¼ 1ï¼Œæœ€å°å€¼ 0ï¼‰
        y = torch.from_numpy(y).permute(0, 4, 3, 1, 2)  # TxHxWx3x1 -> Tx1x3xHxW
        y = y.to(self.device)

        return y    # T x 1 x 3 x H x W   å¸§æ•°ã€æ‰¹æ¬¡ã€ã€ã€

    # æå–å‚è€ƒå›¾åƒçš„å¤–è§‚ç‰¹å¾
    def extract_feature_3d(self, x: torch.Tensor) -> torch.Tensor:
        """ get the appearance feature of the image by F   é€šè¿‡Fè·å–å›¾åƒçš„å¤–è§‚ç‰¹å¾
        x: Bx3xHxW, normalized to 0~1
        """
        with torch.no_grad(), self.inference_ctx():     # å…³é—­æ¢¯åº¦è®¡ç®—
            feature_3d = self.appearance_feature_extractor(x)

        return feature_3d.float()

    # è·å– å•å¼ å›¾åƒï¼ˆå‚è€ƒå›¾åƒorè§†é¢‘å¸§ï¼‰ä¸­ è®¡ç®—éšå¼å…³é”®ç‚¹ç›¸å…³çš„ä¿¡æ¯
    def get_kp_info(self, x: torch.Tensor, **kwargs) -> dict:     # x: (B,3,H=256,W=256)
        """ get the implicit keypoint information
        x: B x 3 x H x W, normalized to 0~1           Bï¼šæ‰¹æ¬¡ï¼ˆå‚è€ƒå›¾åƒçš„ä¸ªæ•°ï¼‰  B=1
        flag_refine_info: whether to trandform the pose to degrees and the dimention of the reshape  æ˜¯å¦ å°†å§¿åŠ¿è½¬æ¢ä¸ºåº¦æ•° å’Œ é‡å¡‘çš„å°ºå¯¸
        return: A dict contains keys: 'pitch', 'yaw', 'roll', 't'ï¼ˆå¹³ç§»ï¼‰, 'exp'ï¼ˆè¡¨æƒ…å˜å½¢ï¼‰, 'scale'ï¼ˆç¼©æ”¾ï¼‰, 'kp'ï¼ˆè§„èŒƒå…³é”®ç‚¹ï¼‰
        """
        with torch.no_grad(), self.inference_ctx():     # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼Œè¿™æ ·åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¸ä¼šè®¡ç®—æ¢¯åº¦ï¼Œä»è€ŒèŠ‚çœå†…å­˜å’ŒåŠ é€Ÿæ¨ç†ã€‚
            kp_info = self.motion_extractor(x)             # è¿åŠ¨æå–å™¨M è¿”å› éšå¼å…³é”®ç‚¹ç›¸å…³çš„ä¿¡æ¯  
            #     kp_info = out = ret_dct = {    ä»Motion Extractorå¾—åˆ°
            #         'pitch': pitch,        # (B, C=66)
            #         'yaw': yaw,            # (B, C=66)
            #         'roll': roll,          # (B, C=66)
            #         't': t,                # (B, C=3)
            #         'exp': exp,            # (B, C=3 * 21)
            #         'scale': scale,        # (B, C=1)
            #         'kp': kp,   # canonical keypoint   (B, C=3 * 21)
            #     }
            if self.inference_cfg.flag_use_half_precision:     # æ ‡å¿—ä½¿ç”¨åŠç²¾åº¦  æ‰€æœ‰çš„å¼ é‡å€¼è½¬æ¢ä¸º float32 ç±»å‹
                # float the dict
                for k, v in kp_info.items():
                    if isinstance(v, torch.Tensor):
                        kp_info[k] = v.float()

        flag_refine_info: bool = kwargs.get('flag_refine_info', True)
        if flag_refine_info:
            bs = kp_info['kp'].shape[0]              # batch sizeï¼šbsï¼šå•ä¸ªæ‰¹æ¬¡çš„å¤§å°
            # é¢„æµ‹å€¼è½¬ä¸ºè§’åº¦          B x 1
            kp_info['pitch'] = headpose_pred_to_degree(kp_info['pitch'])[:, None]  # B x 1    ä¿¯ä»°è§’
            kp_info['yaw'] = headpose_pred_to_degree(kp_info['yaw'])[:, None]  # B x 1        åèˆªè§’
            kp_info['roll'] = headpose_pred_to_degree(kp_info['roll'])[:, None]  # B x 1      æ¨ªæ»šè§’
            #   Nï¼šå…³é”®ç‚¹çš„ä¸ªæ•°ï¼ˆN=68?ï¼‰    3ä»£è¡¨xyzä¸‰ç»´
            # -1è¡¨ç¤ºåœ¨å›ºå®šäº†bså’Œ3åï¼Œæ ¹æ®å‰©ä¸‹çš„å‚æ•°è‡ªåŠ¨è®¡ç®—Nçš„ä¸ªæ•°
            kp_info['kp'] = kp_info['kp'].reshape(bs, -1, 3)  # B x N x 3                     è§„èŒƒå…³é”®ç‚¹
            kp_info['exp'] = kp_info['exp'].reshape(bs, -1, 3)  # B x N x 3                   è¡¨æƒ…å˜æ¢

        return kp_info
            #     kp_info = out = ret_dct = {    ä»Motion Extractorå¾—åˆ°
            #         'pitch': pitch,        # (B, C=1)
            #         'yaw': yaw,            # (B, C=1)
            #         'roll': roll,          # (B, C=1)
            #         't': t,                # (B, C=3)
            #         'exp': exp,            # (B, num_kp = 21, C=3)
            #         'scale': scale,        # (B, C=1)
            #         'kp': kp,   # canonical keypoint   (B, num_kp = 21, C=3)
            #     }

    # é€šè¿‡å§¿åŠ¿ã€ä½ç§»å’Œè¡¨æƒ…å˜å½¢ è®¡ç®— éšå¼å…³é”®ç‚¹
    def transform_keypoint(self, kp_info: dict):
        """
        transform the implicit keypoints with the pose, shift, and expression deformation
        é€šè¿‡å§¿åŠ¿ã€ä½ç§»å’Œè¡¨æƒ…å˜å½¢è½¬æ¢éšå¼å…³é”®ç‚¹
        kp: B x N x 3   Nå³kï¼Œå…³é”®ç‚¹çš„ä¸ªæ•°
        """
        kp = kp_info['kp']    # (bs, k, 3)
        pitch, yaw, roll = kp_info['pitch'], kp_info['yaw'], kp_info['roll']

        t, exp = kp_info['t'], kp_info['exp']           # t: ï¼ˆbsï¼Œ3ï¼‰    expï¼š (bs, k, 3)
        scale = kp_info['scale']                        # (bs, 1)

        pitch = headpose_pred_to_degree(pitch)
        yaw = headpose_pred_to_degree(yaw)
        roll = headpose_pred_to_degree(roll)

        bs = kp.shape[0]
        if kp.ndim == 2:        # (bs, k * 3)
            num_kp = kp.shape[1] // 3  # Bx(num_kp x 3)
        else:
            num_kp = kp.shape[1]  # (bs, k, 3)

        rot_mat = get_rotation_matrix(pitch, yaw, roll)    # (bs, 3, 3)         å³R

        # Eqn.2: s * (R * x_c,s + exp) + t
        kp_transformed = kp.view(bs, num_kp, 3) @ rot_mat + exp.view(bs, num_kp, 3)     # x_c,s * R + exp        (bs, k, 3)
        # ä½¿ç”¨Noneå¢åŠ ä¸€ç»´
        kp_transformed *= scale[..., None]  # (bs, k, 3) * (bs, 1, 1) = (bs, k, 3)      # * s                    (bs, k, 3)
        kp_transformed[:, :, 0:2] += t[:, None, 0:2]  # remove z, only apply tx ty      # + t                    (bs, k, 3)

        return kp_transformed    # éšå¼å…³é”®ç‚¹ (bs, k, 3)   bså›¾åƒä¸ªæ•°ï¼›kå…³é”®ç‚¹ä¸ªæ•°ï¼›3è¡¨ç¤ºxyzä¸‰ç»´

    # é‡å®šå‘å˜´å”‡
    def retarget_lip(self, kp_source: torch.Tensor, lip_close_ratio: torch.Tensor) -> torch.Tensor:
        """
        kp_source: B x N x 3
        lip_close_ratio: B x 2
        Return: B x (3 * num_kp)
        """
        feat_lip = concat_feat(kp_source, lip_close_ratio)

        with torch.no_grad():
            delta = self.stitching_retargeting_module['lip'](feat_lip)

        return delta.reshape(-1, kp_source.shape[1], 3)

    # ç¼åˆ
    def stitch(self, kp_source: torch.Tensor, kp_driving: torch.Tensor) -> torch.Tensor:
        """
        kp_source: BxNx3
        kp_driving: BxNx3
        Return: Bx(3*num_kp+2)
        """
        feat_stiching = concat_feat(kp_source, kp_driving)

        with torch.no_grad():
            delta = self.stitching_retargeting_module['stitching'](feat_stiching)

        return delta

    def stitching(self, kp_source: torch.Tensor, kp_driving: torch.Tensor) -> torch.Tensor:
        """ conduct the stitching
        kp_source: Bxnum_kpx3
        kp_driving: Bxnum_kpx3
        """

        if self.stitching_retargeting_module is not None:

            bs, num_kp = kp_source.shape[:2]

            kp_driving_new = kp_driving.clone()
            delta = self.stitch(kp_source, kp_driving_new)

            delta_exp = delta[..., :3*num_kp].reshape(bs, num_kp, 3)  # 1x20x3
            delta_tx_ty = delta[..., 3*num_kp:3*num_kp+2].reshape(bs, 1, 2)  # 1x1x2

            kp_driving_new += delta_exp
            kp_driving_new[..., :2] += delta_tx_ty

            return kp_driving_new

        return kp_driving

    # æ‰­æ›²è§£ç ï¼ˆå›¾åƒç”Ÿæˆï¼‰     ï¼ï¼ï¼
    def warp_decode(self, feature_3d: torch.Tensor, kp_source: torch.Tensor, kp_driving: torch.Tensor) -> torch.Tensor:
        """ get the image after the warping of the implicit keypoints   è·å–éšå¼å…³é”®ç‚¹æ‰­æ›²åçš„å›¾åƒ
        feature_3d: B x 32 x 16 x 64 x 64, feature volume
        kp_source: B x N x 3
        kp_driving: B x N x 3
        """
        # The line 18 in Algorithm 1: D(W(f_s; x_s, xâ€²_d,i)ï¼‰
        with torch.no_grad(), self.inference_ctx():
            if self.compile:
                # Mark the beginning of a new CUDA Graph step   æ ‡è®°æ–°CUDAå›¾å½¢æ­¥éª¤çš„å¼€å§‹
                torch.compiler.cudagraph_mark_step_begin()
            # get decoder input    è·å–è¢«æ‰­æ›²çš„å›¾åƒ    rec_det['out'] : Bx256x64x64
            ret_dct = self.warping_module(feature_3d, kp_source=kp_source, kp_driving=kp_driving)    # é¢„è®­ç»ƒçš„æ‰­æ›²æ¨¡å— W 
            # decode  ç”Ÿæˆæœ€ç»ˆçš„å›¾åƒ
            ret_dct['out'] = self.spade_generator(feature=ret_dct['out'])   # Bx3x512x512 å½’ä¸€åŒ–åçš„ç”Ÿæˆçš„å›¾åƒ                   # é¢„è®­ç»ƒçš„ç”Ÿæˆå™¨ G

            # float the dict æ”¹å˜å‚æ•°ç±»å‹
            if self.inference_cfg.flag_use_half_precision:
                for k, v in ret_dct.items():
                    if isinstance(v, torch.Tensor):
                        ret_dct[k] = v.float()

        return ret_dct   
        # å…¶ä¸­ ret_dct = {
        #     'occlusion_map': occlusion_map,      # Bx1x64x64        é®æŒ¡å›¾
        #     'deformation': deformation,          # Bx16x64x64x3     ç»„åˆæµåœº
        #     'out': out,                          # Bx3x512x512      ç”Ÿæˆçš„å½’ä¸€åŒ–çš„å›¾åƒ0~1
        # }

    # æ”¹å˜è¾“å‡ºçš„æ ¼å¼
    def parse_output(self, out: torch.Tensor) -> np.ndarray:
        """ construct the output as standard   å°†è¾“å‡ºæ„å»ºä¸ºæ ‡å‡†
        outï¼š 1x3xHxWï¼Œæ ‡å‡†åŒ–åˆ°0~1åçš„      Bx3x512x512
        return: 1xHxWx3, uint8
        """
        out = np.transpose(out.data.cpu().numpy(), [0, 2, 3, 1])  # 1x3xHxW -> 1xHxWx3
        out = np.clip(out, 0, 1)  # clip to 0~1
        out = np.clip(out * 255, 0, 255).astype(np.uint8)  # 0~1 -> 0~255

        return out  # Bx512x512x3

    def calc_ratio(self, lmk_lst):
        '''è®¡ç®—çœ¼ç›å˜´å·´closeçš„æ¯”ä¾‹'''
        input_eye_ratio_lst = []
        input_lip_ratio_lst = []
        for lmk in lmk_lst:
            # for eyes retargeting
            input_eye_ratio_lst.append(calc_eye_close_ratio(lmk[None]))
            # for lip retargeting
            input_lip_ratio_lst.append(calc_lip_close_ratio(lmk[None]))
        return input_eye_ratio_lst, input_lip_ratio_lst

    # è®¡ç®—ç»“åˆçœ¼ç›çš„æ¯”ä¾‹
    def calc_combined_eye_ratio(self, c_d_eyes_i, source_lmk):
        c_s_eyes = calc_eye_close_ratio(source_lmk[None])
        c_s_eyes_tensor = torch.from_numpy(c_s_eyes).float().to(self.device)
        c_d_eyes_i_tensor = torch.Tensor([c_d_eyes_i[0][0]]).reshape(1, 1).to(self.device)
        # [c_s,eyes, c_d,eyes,i]
        combined_eye_ratio_tensor = torch.cat([c_s_eyes_tensor, c_d_eyes_i_tensor], dim=1)
        return combined_eye_ratio_tensor

    # è®¡ç®—å”‡éƒ¨å¼ å¼€åº¦æ¯”ä¾‹
    def calc_combined_lip_ratio(self, c_d_lip_i, source_lmk):
        c_s_lip = calc_lip_close_ratio(source_lmk[None])
        c_s_lip_tensor = torch.from_numpy(c_s_lip).float().to(self.device)
        c_d_lip_i_tensor = torch.Tensor([c_d_lip_i[0]]).to(self.device).reshape(1, 1) # 1x1
        # [c_s,lip, c_d,lip,i]
        combined_lip_ratio_tensor = torch.cat([c_s_lip_tensor, c_d_lip_i_tensor], dim=1) # 1x2
        return combined_lip_ratio_tensor

    # ï¼ˆæ ¹æ®éŸ³é¢‘ï¼‰ç”Ÿæˆè¿åŠ¨åºåˆ—ï¼ï¼ï¼
    def gen_motion_sequence(self, args):
       # preprocess audio    å¤„ç†éŸ³é¢‘
        log(f"start loading audio from {args.audio}")
        audio, _ = librosa.load(args.audio, sr=16000, mono=True)  # ä»ç»™å®šè·¯å¾„åŠ è½½éŸ³é¢‘ï¼Œè®¾ç½®é‡‡æ ·ç‡ä¸º 16,000 Hzï¼Œmono=Trueè¡¨ç¤ºå°†éŸ³é¢‘è½¬æ¢ä¸ºå•å£°é“
        # audio shape: (æ€»é‡‡æ ·æ—¶é•¿, )
        log(f"audio loaded! {audio.shape}")
        if isinstance(audio, np.ndarray):       # (n_samples,)   
            audio = torch.from_numpy(audio).to(self.device)         # è½¬æˆtorchå¼ é‡ç±»å‹  [n_samples]    eg:[170528]
        assert audio.ndim == 1, 'Audio must be 1D tensor.'          # ç¡®ä¿éŸ³é¢‘æ˜¯ä¸€ä¸ªä¸€ç»´å¼ é‡ï¼ˆå³å•é€šé“éŸ³é¢‘ï¼‰
        log(f"loading audio from: {args.audio}")
        audio = F.pad(audio, (1280, 640), "constant", 0)     # F.pad: å¯¹éŸ³é¢‘è¿›è¡Œå¡«å……ï¼Œå·¦ä¾§å¡«å…… 1280 ä¸ªæ ·æœ¬ï¼Œå³ä¾§å¡«å…… 640 ä¸ªæ ·æœ¬ã€‚å¡«å……å€¼ä¸º 0ã€‚  eg: [170528] -> [172448]

        # crop audio into n_subdivision according to n_motions   
        # æ ¹æ®n_motionsï¼ˆå¸§/ç‰‡æ®µæ•°ï¼‰å°†éŸ³é¢‘è£å‰ªä¸ºn_subdivisionï¼ˆç‰‡æ®µæ•°ï¼‰ä»½
        clip_len = int(len(audio) / 16000 * self.fps)       # len(audio) / 16000ä¸ºéŸ³é¢‘æ—¶é•¿ï¼›clip_lenä¸ºæ€»å¸§æ•°     eg:269
        stride = self.n_motions                   # æ­¥é•¿ï¼ˆæ¯ä¸ªéŸ³é¢‘ç‰‡æ®µçš„å¸§æ•°ï¼‰
        if clip_len <= self.n_motions:
            n_subdivision = 1                               # æ€»å¸§æ•°å°äºæ­¥é•¿ï¼Œåªåˆ†æˆä¸€ä»½
        else:
            n_subdivision = math.ceil(clip_len / stride)    # æŒ‰æ­¥é•¿åˆ†æˆn_subdivisionä»½      eg:3

        # padding  å¡«å……
        n_padding_audio_samples = self.n_audio_samples * n_subdivision - len(audio)  # è®¡ç®—éŸ³é¢‘éœ€è¦å¡«å……çš„æ ·æœ¬æ•°é‡ï¼Œä»¥ä¿è¯éŸ³é¢‘é•¿åº¦ä¸ n_subdivision å¯¹åº”çš„è¿åŠ¨ç”Ÿæˆæ ·æœ¬æ•°é‡åŒ¹é…   eg:  n_padding_audio_samples = 19552
        n_padding_frames = math.ceil(n_padding_audio_samples / self.audio_unit)           # éœ€è¦å¡«å……çš„å¸§æ•°
        if n_padding_audio_samples > 0:          # éœ€è¦å¡«å……çš„æ ·æœ¬æ•°é‡   eg: 19552
            if self.pad_mode == 'zero':               # ä½¿ç”¨ 0 å¡«å……
                padding_value = 0
            elif self.pad_mode == 'replicate':        # å¤åˆ¶æœ€åä¸€ä¸ªéŸ³é¢‘æ ·æœ¬å¡«å……
                padding_value = audio[-1]
            else:
                raise ValueError(f'Unknown pad mode: {self.pad_mode}')
            audio = F.pad(audio, (0, n_padding_audio_samples), value=padding_value)     #  172448 + 19552 = 192000 = self.n_audio_samples * n_subdivision
            # å…¶ä¸­ n_audio_samplesï¼šn_motions=100å¸§å¯¹åº”çš„é‡‡æ ·æ•°   ï¼›  n_subdivisionï¼šåˆ†æˆäº†å¤šå°‘ä¸ªn_motions=100å¸§
            # å·¦ä¾§å¡«å……0ä¸ªæ ·æœ¬ï¼Œå³ä¾§å¡«å…… n_padding_audio_samples ä¸ªæ ·æœ¬ã€‚å¡«å……å€¼ä¸º padding_valueã€‚  shapeï¼š(n_samples,)

        # generate motions
        coef_list = []
        for i in range(0, n_subdivision):              # éŸ³é¢‘çš„ä»½æ•°ï¼ˆå­åºåˆ—ï¼‰ï¼Œæ¯ä¸€ä»½åŒ…å« stride = n_motions=100å¸§ã€‚
            start_idx = i * stride                     # è¿™ä»½ èµ·å§‹ å¸§çš„ç´¢å¼•
            end_idx = start_idx + self.n_motions       # è¿™ä»½ ç»“æŸ å¸§çš„ç´¢å¼•
            indicator = torch.ones((1, self.n_motions)).to(self.device) if self.use_indicator else None  # æŒ‡ç¤ºå™¨ï¼ˆå…¨ 1 çš„å¼ é‡ï¼‰ï¼Œç”¨äºåœ¨æœ€åä¸€ä¸ªå­åºåˆ—ä¸Šæ ‡è®°å¡«å……éƒ¨åˆ†ã€‚(1, self.n_motions=100)
            if indicator is not None and i == n_subdivision - 1 and n_padding_frames > 0:
                indicator[:, -n_padding_frames:] = 0     # æŒ‡ç¤ºå™¨ï¼ˆçœŸå®éŸ³é¢‘ä¸ºTrueï¼Œå¡«å……éƒ¨åˆ†ä¸º0ï¼‰ï¼Œç”¨äºåœ¨æœ€åä¸€ä¸ªå­åºåˆ—ä¸Šæ ‡è®° å¡«å……çš„éƒ¨åˆ†ã€‚
            audio_in = audio[round(start_idx * self.audio_unit):round(end_idx * self.audio_unit)].unsqueeze(0)  # audio_in: æ ¹æ®éŸ³é¢‘çš„å­åºåˆ—ç´¢å¼•æå–éŸ³é¢‘ç‰‡æ®µï¼Œå¹¶å¢åŠ ä¸€ä¸ªç»´åº¦ä»¥å½¢æˆ (1, n_samples_in_subdivision) çš„å½¢çŠ¶ã€‚
            # shape: [1, self.n_audio_samples = audio_len / n_subdivision]   eg:[1, 64000]

### --------#  è¿™é‡Œéœ€è¦åŠ ä¸€ä¸ªä»éŸ³é¢‘ä¸­åˆ†ææƒ…æ„Ÿ # --------------------------
            emo_index = torch.tensor(emo_list.index(args.emotype))    # emoå¯¹åº”çš„ç´¢å¼•å€¼    # (B=1,)
            emo_index = emo_index.unsqueeze(0).to(self.device)   # æ‰©å±•ç»´åº¦å¹¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆä¾‹å¦‚ GPUï¼‰  
  
            if i == 0:    # ç¬¬ä¸€ä¸ªç‰‡æ®µæ²¡æœ‰å…ˆå‰å¸§çš„æŒ‡å¯¼
                # è°ƒç”¨ motion_generator.sample ç”Ÿæˆè¿åŠ¨ç‰¹å¾ã€‚è¿”å›çš„ motion_feat æ˜¯ç”Ÿæˆçš„è¿åŠ¨ç‰¹å¾ï¼Œnoise æ˜¯å™ªå£°ï¼Œprev_audio_feat æ˜¯å…ˆå‰çš„éŸ³é¢‘ç‰¹å¾ã€‚
                motion_feat, noise, prev_audio_feat = self.motion_generator.sample(audio_in,        # (1, n_motion=100ï¼ˆå³frame per seq), n_features=73) - æ¯ä¸ªå­åºåˆ—ç”Ÿæˆçš„è¿åŠ¨ç‰¹å¾
                                                                        indicator=indicator, cfg_mode=args.cfg_mode,
                                                                        cfg_cond=args.cfg_cond, cfg_scale=args.cfg_scale,
                                                                        dynamic_threshold=0,emo_index=emo_index)
            else:
                motion_feat, noise, prev_audio_feat = self.motion_generator.sample(audio_in,
                                                                        prev_motion_feat, prev_audio_feat, noise,     # éœ€è¦æ·»åŠ å…ˆå‰çš„ è¿åŠ¨ç‰¹å¾ï¼ŒéŸ³é¢‘ç‰¹å¾ï¼Œå™ªå£°
                                                                        indicator=indicator, cfg_mode=args.cfg_mode,
                                                                        cfg_cond=args.cfg_cond, cfg_scale=args.cfg_scale,
                                                                        dynamic_threshold=0,emo_index=emo_index)
            prev_motion_feat = motion_feat[:, -self.n_prev_motions:].clone()        # copyä¸´è¿‘çš„n_prev_motions=0å¸§ï¼Œå…ˆå‰çš„è¿åŠ¨ç‰¹å¾  (N=1, L=10, motion_feat_dim=73)
            prev_audio_feat = prev_audio_feat[:, -self.n_prev_motions:]             # copyï¼Œä½œä¸ºå…ˆå‰çš„éŸ³é¢‘ç‰¹å¾                      (N=1, L=10, feature_dim=256)

#### 20250420 æƒ…æ„Ÿå¢å¼º
            if self.emo_ehance:
                emo_level = torch.tensor([args.enhance_level-1],dtype=torch.long).to(self.device)
                delta_emo = self.emo_enhancer(motion_feat[:, self.n_prev_motions:, :63], emo_index, emo_level)
                motion_feat[:, self.n_prev_motions:, :63] = motion_feat[:, self.n_prev_motions:, :63] + delta_emo.detach()
#### ------------------------
            motion_coef = motion_feat         #  è¿åŠ¨ç³»æ•° "coefficient"ï¼ˆç³»æ•°ï¼‰  (N=1, L=100, motion_feat_dim=73)
            if i == n_subdivision - 1 and n_padding_frames > 0:         # æœ€åä¸€æ®µéŸ³é¢‘åºåˆ—
                motion_coef = motion_coef[:, :-n_padding_frames]  # delete padded frames   åˆ é™¤å³ä¾§æ–°å¢çš„n_padding_frameså¸§  (N=1, L - n_padding_frames, motion_feat_dim=73)
            coef_list.append(motion_coef)   # len = 3
            motion_coef = torch.cat(coef_list, dim=1)                # (1, n_framesè§†é¢‘æ€»å¸§æ•°, n_features=73) - æ•´ä¸ªè¿åŠ¨åºåˆ—çš„è¿åŠ¨ç³»æ•°      n_frames = n_motions * n_subdivision
            # motion_coef = self.reformat_motion(args, motion_coef)

        motion_coef = motion_coef.squeeze() #.cpu().numpy().astype(np.float32)     # å»é™¤å¼ é‡ä¸­æ‰€æœ‰å°ºå¯¸ä¸º1çš„ç»´åº¦ã€‚(n_frames, n_features=70)
        motion_list = []              # è¿åŠ¨åˆ—è¡¨
        # Emotion_template_dict = self.templete_dict[emo_list.index(args.emotype)]
        Emotion_template_dict = self.templete_dict
        for idx in track(range(motion_coef.shape[0]), description='ğŸš€Generating Motion Sequence...', total=motion_coef.shape[0]):    # æ€»å¸§æ•°
            # æŒ‰ç…§æ¨¡æ¿å­—å…¸ä¸­çš„æ ‡å‡†å·®å’Œå‡å€¼è¿›è¡Œåå½’ä¸€åŒ–ï¼ˆä» 0~1 åˆ°å„è‡ªçš„èŒƒå›´ï¼‰
            exp = motion_coef[idx][:63].cpu() * Emotion_template_dict["std_exp"] + Emotion_template_dict["mean_exp"]    # [63]
            scale = motion_coef[idx][63:64].cpu() * (Emotion_template_dict["max_scale"] - Emotion_template_dict["min_scale"]) + Emotion_template_dict["min_scale"]   # [1]
            t = motion_coef[idx][64:67].cpu() * (Emotion_template_dict["max_t"] - Emotion_template_dict["min_t"]) + Emotion_template_dict["min_t"]    # [3]
            pitch = motion_coef[idx][67:68].cpu() * (Emotion_template_dict["max_pitch"] - Emotion_template_dict["min_pitch"]) + Emotion_template_dict["min_pitch"]   # [1]
            yaw = motion_coef[idx][68:69].cpu() * (Emotion_template_dict["max_yaw"] - Emotion_template_dict["min_yaw"]) + Emotion_template_dict["min_yaw"]   # [1]
            roll = motion_coef[idx][69:70].cpu() * (Emotion_template_dict["max_roll"] - Emotion_template_dict["min_roll"]) + Emotion_template_dict["min_roll"]   # [1]

            R = get_rotation_matrix(pitch, yaw, roll)                    # æ—‹è½¬çŸ©é˜µ
            R = R.reshape(1, 3, 3).cpu().numpy().astype(np.float32)           # (1, 3, 3)     1ä»£è¡¨åªæœ‰ä¸€å¼ å›¾ç‰‡    3*3çš„æ—‹è½¬çŸ©é˜µ
            
            exp = exp.reshape(1, 21, 3).cpu().numpy().astype(np.float32)      # (1, 21, 3)    21ä¸ªè¡¨æƒ…å…³é”®ç‚¹     
            scale = scale.reshape(1, 1).cpu().numpy().astype(np.float32)      # (1, 1)
            t = t.reshape(1, 3).cpu().numpy().astype(np.float32)              # (1, 3)         x,y,zä¸‰ä¸ªåæ ‡
            pitch = pitch.reshape(1, 1).cpu().numpy().astype(np.float32)      # (1, 1)         æ—‹è½¬å¯¹å…¨å±€ï¼ˆæ•´ä¸ªå¤´éƒ¨ï¼‰æœ‰æ•ˆï¼Œå› æ­¤åªæœ‰ä¸€ä¸ªå€¼
            yaw = yaw.reshape(1, 1).cpu().numpy().astype(np.float32)          # (1, 1)
            roll = roll.reshape(1, 1).cpu().numpy().astype(np.float32)        # (1, 1)
            
            motion_list.append({"exp": exp, "scale": scale, "R": R, "t": t, "pitch": pitch, "yaw": yaw, "roll": roll})
        tgt_motion = {'n_frames': motion_coef.shape[0], 'output_fps': 25, 'motion': motion_list}    # å¸§æ•°ã€‚å¸§ç‡ã€‚è¿åŠ¨å‚æ•°åˆ—è¡¨ã€‚

        if args.is_smooth_motion:      # å¹³æ»‘è¿åŠ¨
            tgt_motion = smooth_(tgt_motion, method="ema")      # ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰æ–¹æ³•å¯¹è¿åŠ¨è¿›è¡Œå¹³æ»‘
        return tgt_motion
    