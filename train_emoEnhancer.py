import logging

from src.dataset.dataset_emoEnhancer import DiT_Emo_Dataset
from src.modules.emotion_enhancer import EmotionTransformer
import torch
import torch.nn as nn
import pickle
import numpy as np
import torch.optim as optim
from torch.utils import data
from tensorboardX import SummaryWriter
from src.dataset import infinite_data_loader
from src.scheduler import GradualWarmupScheduler
from src.ADEF_wrapper import ADEFWrapper
torch.backends.cudnn.benchmark = True # disable CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR warning

import cv2; cv2.setNumThreads(0); cv2.ocl.setUseOpenCL(False)
import os.path as osp
 
from rich.progress import track

from src.config.argument_config import ArgumentConfig
from src.config.inference_config import InferenceConfig
from src.utils.camera import get_rotation_matrix
from src.utils.video import images2video, add_audio_to_video
from src.utils.io import load_image_rgb, resize_to_limit
from src.utils.helper import mkdir, basename, dct2device
from src.utils.rprint import rlog as log
import tyro
from src.modules.emotion_level_classifier import EmotionTransformer as Classifier

def partial_fields(target_class, kwargs):
    return target_class(**{k: v for k, v in kwargs.items() if hasattr(target_class, k)})


emo_list = ['angry', 'contempt', 'disgusted', 'fear', 'happy', 'neutral', 'sad', 'surprised']
emo_label = ['ang',  'con',  'dis',  'fea',  'hap',  'neu',  'sad',  'sur']

def transf_train():
    # è®­ç»ƒè¶…å‚æ•°
    num_epochs = 120000
    learning_rate = 1e-4

    warm_iter = 12000
    decay_iter = 120000
    batch_size = 64
    # åŒåˆ†å¸ƒ
    log_dir = f'./0511_64_120000_gt15å¢å¼º'
    
    writer = SummaryWriter(log_dir)   # è·¯å¾„

    # åˆå§‹åŒ–æ¨¡å‹
    device = torch.device("cuda:3" if torch.cuda.is_available() else "cpu")
    model = EmotionTransformer().to(device)

    # 0417 æƒ…æ„Ÿåˆ†ç±»å™¨    0426é«˜dimä¸”åŒåˆ†å¸ƒ
    emo_classifier = Classifier().to(device)
    emo_classifier.load_state_dict(torch.load(f'/mnt/disk2/zhouxishi/JoyVASA/pretrained_weights/ADEF/emo_classifier/emo_level_classifier.pth', map_location=device))
    emo_classifier.eval()
    criterion = torch.nn.CrossEntropyLoss()

    # å®šä¹‰æŸå¤±å‡½æ•°
    mse_loss = nn.MSELoss()  # ä¸»è¦æŸå¤±ï¼Œçº¦æŸè¿åŠ¨å‚æ•°
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    after_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, decay_iter, learning_rate * 0.02)
    scheduler = GradualWarmupScheduler(optimizer, 1, warm_iter, after_scheduler)

    train_dataset = DiT_Emo_Dataset()
    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    train_loader = infinite_data_loader(train_loader)   # å°†æ•°æ®åŠ è½½å™¨ï¼ˆtrain_loaderï¼‰è½¬æ¢ä¸ºä¸€ä¸ªæ— é™å¾ªç¯çš„è¿­ä»£å™¨

    model.train()
    loss_log = {
        'total_loss': [],
        'mse_loss': []
        ,'emo_loss':[]
        ,'level_loss': []
    }

    for epoch in range(num_epochs):
        dit_prev, _, gt_prev, _, emo_index, emo_level = next(train_loader)
        dit_prev = dit_prev.to(device)     # ï¼ˆB,100,63ï¼‰
        # dit_cur = dit_cur.to(device)      # ï¼ˆB,100,63ï¼‰
        emo_index = emo_index.to(device)     # ï¼ˆB,ï¼‰
        gt_prev = gt_prev.to(device)       # ï¼ˆB,100,63ï¼‰
        # gt_cur = gt_cur.to(device)       # ï¼ˆB,100,63ï¼‰
        emo_level = emo_level.to(device)

        optimizer.zero_grad()

        pred = model(dit_prev, emo_index, emo_level)     #   (B, 1, 63)

        B, L, _ = dit_prev.shape
        dit_prev = dit_prev + pred.expand(-1, L, -1)   # (B, 1, 63)  -> (B, L, 63)

        # === è®¡ç®—å„ä¸ªæŸå¤± ===
        loss_mse = mse_loss(dit_prev, gt_prev)  # (B, 100, 63)

        pred_emo, pred_level = emo_classifier(dit_prev)   # (N,100,63)  -> (N,8)
        loss_emo = criterion(pred_emo, emo_index)
        loss_level = criterion(pred_level, emo_level)

        # === è®¡ç®—æ€»æŸå¤± ===
        total_loss = loss_mse + loss_emo + loss_level

        # åå‘ä¼ æ’­
        total_loss.backward()
        optimizer.step()

        # Inside your training loop, after computing the losses:
        # Logging - Append current batch losses
        loss_log['total_loss'].append(total_loss.mean().item())
        loss_log['mse_loss'].append(loss_mse.mean().item())
        loss_log['emo_loss'].append(loss_emo.mean().item())
        loss_log['level_loss'].append(loss_emo.mean().item())

        # Create description string for logging
        description = f'Iter: {epoch}\t Train loss: [Total: {np.mean(loss_log["total_loss"]):.3e}'
        description += f", MSE: {np.mean(loss_log['mse_loss']):.3e}"
        description += f", Emo: {np.mean(loss_log['emo_loss']):.3e}"
        description += f", Level: {np.mean(loss_log['level_loss']):.3e}"
        description += ']'
        logging.info(description)

        # Write to tensorboard
        if epoch % 50 == 0 and writer is not None:
            writer.add_scalar('train/total_loss', np.mean(loss_log['total_loss']), epoch)
            writer.add_scalar('train/mse_loss', np.mean(loss_log['mse_loss']), epoch)
            writer.add_scalar('train/emo_loss', np.mean(loss_log['emo_loss']), epoch)
            writer.add_scalar('train/level_loss', np.mean(loss_log['level_loss']), epoch)
            writer.add_scalar('opt/lr', optimizer.param_groups[0]['lr'], epoch)
            
            # Clear the loss log for next interval
            for key in loss_log.keys():
                loss_log[key].clear()

        # update learning rate  æ›´æ–°å­¦ä¹ ç‡
        if scheduler is not None and epoch < warm_iter + decay_iter:   # è°ƒåº¦å™¨ç”¨äºæ›´æ–°å­¦ä¹ ç‡  åŒºåˆ†ï¼šä¼˜åŒ–å™¨optimizor
            scheduler.step()

        # æ‰“å°è®­ç»ƒä¿¡æ¯
        if (epoch + 1) % 50 == 0:
            print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.item():.6f}")

    print("è®­ç»ƒå®Œæˆï¼")
    save_dir = f"{log_dir}/ckpt.pth"
    torch.save(model.state_dict(), save_dir)

def infer(emo_le = 2):
    tyro.extras.set_accent_color("bright_cyan")
    args = tyro.cli(ArgumentConfig)
    inf_cfg = partial_fields(InferenceConfig, args.__dict__)    # ä»args.__dict__é€‰å–InferenceConfigå¯¹åº”çš„å­—æ®µ ç»„æˆçš„å­—å…¸
    adef_wrapper = ADEFWrapper(inference_cfg=inf_cfg)       # æ ¸å¿ƒåŠŸèƒ½åŒ…è£…å™¨
    device = adef_wrapper.device          # cuda:0

    transf_model = EmotionTransformer().to(device)
    enhancer_p = '/mnt/disk2/zhouxishi/ADEF/pretrained_weights/ADEF/emo_enhancer/emo_enhancer.pth'
    transf_model_data = torch.load(enhancer_p, map_location=device)
    transf_model.load_state_dict(transf_model_data, strict=False)   # ['model']
    transf_model.eval()
    templates_dict = pickle.load(open('/mnt/disk2/zhouxishi/ADEF/pretrained_weights/ADEF/motion_template/motion_template.pkl', 'rb'))

    args.output_dir = f'/mnt/disk2/zhouxishi/ADEF'
    emo_level = torch.tensor(emo_le, dtype=torch.long).unsqueeze(0).to(device)
    image = '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/first_frame/M003_front_happy_level_3_027.png'
    audio = '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/videos/M003/front/happy/level_3/M003_front_happy_level_3_027.wav'
    test_datas = [(image, audio)]

    image_list = ['/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/first_frame/M003_front_angry_level_1_001.png',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/first_frame/M003_front_contempt_level_1_001.png',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/first_frame/M003_front_disgusted_level_1_001.png',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/first_frame/M003_front_fear_level_1_001.png',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/first_frame/M003_front_happy_level_1_001.png',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/first_frame/M003_front_neutral_level_1_001.png',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/first_frame/M003_front_sad_level_1_001.png',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/first_frame/M003_front_surprised_level_1_001.png',]

    audio_list = ['/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/videos/M003/front/angry/level_3/M003_front_angry_level_3_001.wav',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/videos/M003/front/contempt/level_3/M003_front_contempt_level_3_001.wav',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/videos/M003/front/disgusted/level_3/M003_front_disgusted_level_3_001.wav',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/videos/M003/front/fear/level_3/M003_front_fear_level_3_001.wav',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/videos/M003/front/happy/level_3/M003_front_happy_level_3_001.wav',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/videos/M003/front/neutral/level_1/M003_front_neutral_level_1_001.wav',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/videos/M003/front/sad/level_3/M003_front_sad_level_3_001.wav',
                  '/mnt/disk2/zhouxishi/JoyVASA/dataset/MEAD11/videos/M003/front/surprised/level_3/M003_front_surprised_level_3_001.wav',]
    # test_datas = [(image_list[i],audio_list[i]) for i in range(len(audio_list))]

    for image,audio in test_datas:
    # for audio in audio_list:
        args.audio = audio
    ######## load reference image  åŠ è½½å‚è€ƒå›¾åƒ########
        img_rgb = load_image_rgb(image)       # (h,w,3)   (336,336,3)
        # è°ƒæ•´å›¾åƒçš„å¤§å°ï¼Œä½¿æœ€å¤§å°ºå¯¸ä¸è¶…è¿‡max_dimï¼Œå›¾åƒçš„å®½åº¦å’Œé«˜åº¦æ˜¯nï¼ˆdivisionï¼‰çš„å€æ•°ã€‚
        img_rgb = resize_to_limit(img_rgb, inf_cfg.source_max_dim, inf_cfg.source_division)       # (h,w,3)   (336,336,3)
        log(f"Load reference image from {image}")
        source_rgb_lst = [img_rgb]    # æºrgbå›¾åƒ åˆ—è¡¨ï¼Œsource_rgb_lst[0]å°±æ˜¯img_rgb    len==1
        
    #########  åŸæœ¬å®ç° #########################
        driving_template_dct = adef_wrapper.gen_motion_sequence(args)
        n_frames = driving_template_dct['n_frames']                        # ï¼ˆéŸ³é¢‘å¯¹åº”çš„ï¼‰æ€»å¸§æ•°

        emo_list = ['angry', 'contempt', 'disgusted', 'fear', 'happy', 'neutral', 'sad', 'surprised']
        emotype = audio.split('/')[-3]
        emo_id = int(emo_list.index(emotype))
        print(emo_id)
        # template_dict = templates_dict[emo_id]
        template_dict = templates_dict
        batch, least = n_frames//100, n_frames % 100        # å…ˆå†™æ­»100å¸§

        ori_exp = []
        for i in range(n_frames):
            exp = driving_template_dct['motion'][i]['exp']   # (1,21,3)
            exp = (exp.flatten() - template_dict["mean_exp"]) / (template_dict["std_exp"] + 1e-9)   # (63)
            exp = torch.tensor(exp, dtype=torch.float32).to(device)  # (63)
            ori_exp.append(exp)
        if least > 0:
            batch += 1
            addi = 100 - least
            for i in range(addi):
                ori_exp.append(ori_exp[-1])
        ori_exp =  torch.stack(ori_exp, dim=0).to(device)      # (batch * 100, 63)
        
        ## (n,1,63)
        # dit_cur = ori_exp[0:100].unsqueeze(0).to(device)       # (1, 100, 63)  
        # emo_index = torch.tensor([emo_id], dtype=torch.long).to(device)     # (1)
        # pred_delta_exp = transf_model(dit_cur,emo_index)    # (1, 1, 63)
        # pred_delta_exp = pred_delta_exp.cpu().detach() * template_dict["std_exp"] + template_dict["mean_exp"]    # [63]
        # pred_delta_exp = pred_delta_exp.reshape(21, 3).unsqueeze(0)                 #   (1,21,3)

        ## (n,100,63)
        res_exp = []
        for i in range(batch):
            dit_cur = ori_exp[i*100:(i+1)*100].unsqueeze(0).to(device)       # (1, 100, 63)  
            emo_index = torch.tensor([emo_id], dtype=torch.long).to(device)     # (1)
            pred_delta_exp = transf_model(dit_cur,emo_index,emo_level)    # (1, 1, 63)
            res_exp.append(pred_delta_exp)          # (1, 100, 63)
        res_exp = torch.stack(res_exp, dim=1).squeeze()     # (1, batch*100, 63) ->  (batch*100, 63)       # (b,25,63) -  (b*25,63)  -  (b*25,21,3) -  (batch*25, 1, 21, 3)
        exp_all = torch.zeros(1,21,3)
        for i in range(n_frames):
            # åå½’ä¸€åŒ–
            exp =  res_exp[i].cpu().detach() * template_dict["std_exp"] + template_dict["mean_exp"]    # [63]
            exp = exp.reshape(21, 3).unsqueeze(0)                 #   (1,21,3)
            exp_all += exp
        exp_all = exp_all / n_frames

        for i in range(n_frames):
            # driving_template_dct['motion'][i]['exp'] += 1 * pred_delta_exp.numpy() #  (1, 21, 3)
            driving_template_dct['motion'][i]['exp'] += 1 * exp_all.numpy()  #  (1, 21, 3)
        
        I_p_lst = []                                      # ç”Ÿæˆçš„è§†é¢‘çš„å›¾åƒåˆ—è¡¨ï¼Œè¿˜æ²¡æœ‰ç²˜è´´åˆ°åŸå›¾ç©ºé—´
        R_d_0, x_d_0_info = None, None             # ç¬¬ä¸€å¸§çš„æ—‹è½¬çŸ©é˜µï¼ˆR_d_0ï¼‰å’Œç›¸åº”çš„è¿åŠ¨ä¿¡æ¯ï¼ˆx_d_0_infoï¼‰ ï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿ

    ######## process source info å¤„ç†æºå‚è€ƒå›¾åƒç›¸å…³çš„ä¿¡æ¯ ########
        img_crop_256x256 = cv2.resize(source_rgb_lst[0], (256, 256))  # force to resize to 256x256  å¼ºåˆ¶è°ƒæ•´å¤§å°ä¸º256x256   ï¼ˆ256,256,3ï¼‰
        
        I_s = adef_wrapper.prepare_source(img_crop_256x256)         # å¤„ç†åçš„å›¾åƒ  H x W x 3  ->  B x 3 x H x W
        x_s_info = adef_wrapper.get_kp_info(I_s)                    # å‚è€ƒå›¾åƒçš„è®¡ç®—éšå¼å…³é”®ç‚¹æ—¶ç›¸å…³çš„ä¿¡æ¯
        x_c_s = x_s_info['kp']                                                    # æºå›¾åƒçš„è§„èŒƒå…³é”®ç‚¹Xc,s      B x 21 x 3 
        R_s = get_rotation_matrix(x_s_info['pitch'], x_s_info['yaw'], x_s_info['roll'])    # æ—‹è½¬çŸ©é˜µR         (B, 3, 3)
        f_s = adef_wrapper.extract_feature_3d(I_s)                  # å‚è€ƒå›¾åƒçš„å¤–è§‚ç‰¹å¾ï¼ˆé¢„è®­ç»ƒçš„Fï¼‰   [1, 32, 16, 64, 64]
        x_s = adef_wrapper.transform_keypoint(x_s_info)             # è¨ˆç®—æºéšå¼å…³é”®ç‚¹Xs,k       (bs, k, 3)  B x 21 x 3 

    ######## animate åŠ¨ç”»åŒ–ï¼ˆé€å¸§å¤„ç†éŸ³é¢‘ï¼‰########
        for i in track(range(n_frames), description='ğŸš€Animating Image with Generated Motions...', total=n_frames):
            x_d_i_info = driving_template_dct['motion'][i]   # åŒ…æ‹¬è¯¥å¸§çš„exp, scale, R, t, pitch, yaw, roll
            x_d_i_info = dct2device(x_d_i_info, device)
            
            # R  æ—‹è½¬çŸ©é˜µ
            R_d_i = x_d_i_info['R'] if 'R' in x_d_i_info.keys() else x_d_i_info['R_d']  # compatible with previous keys ä¸ä»¥å‰çš„é”®å…¼å®¹
            if i == 0:  # cache the first frame     ç¼“å­˜ç¬¬ä¸€å¸§
                R_d_0 = R_d_i                      # ç¬¬ä¸€å¸§çš„æ—‹è½¬çŸ©é˜µ
                x_d_0_info = x_d_i_info.copy()     # ç¬¬ä¸€å¸§çš„exp, scale, R, t, pitch, yaw, rollä¿¡æ¯
            if inf_cfg.animation_region == "all" or inf_cfg.animation_region == "pose":       # "exp", "pose", "lip", "eyes", "all"
                R_new = (R_d_i @ R_d_0.permute(0, 2, 1)) @ R_s  # Rd_i * Rd_0 * Rs    (B=1, 3, 3)
            else:        # "exp", "lip", "eyes"
                R_new = R_s           # ç›´æ¥copyå‚è€ƒå›¾åƒçš„æ—‹è½¬çŸ©é˜µ (B=1, 3, 3)

            # delta  è¡¨æƒ…å˜åŒ– ï¼ˆå¤§é“è‡³ç®€ï¼‰
            delta_new = x_d_i_info['exp']

            # scale  ç¼©æ”¾
            scale_new = x_s_info['scale'] * (x_d_i_info['scale'] / x_d_0_info['scale'])   # ï¼ˆ1,1ï¼‰

            # translation  å¤´éƒ¨å¹³ç§»
            t_new = x_s_info['t'] + (x_d_i_info['t'] - x_d_0_info['t'])    # åŸå›¾å¹³ç§»+ç›¸å¯¹é¦–å¸§å¹³ç§»    ï¼ˆ1,3ï¼‰
            t_new[..., 2].fill_(0)  # zero tz        zåæ ‡å˜ä¸º0

            x_d_i_new = scale_new * (x_c_s @ R_new + delta_new) + t_new         # è®¡ç®—è¯¥å¸§çš„éšå¼å…³é”®ç‚¹   ï¼ˆ1,21,3ï¼‰

            # æ‰­æ›²è§£ç ï¼ˆå›¾åƒç”Ÿæˆï¼‰ï¼šå¤–è§‚ç‰¹å¾ï¼Œæºéšå¼å…³é”®ç‚¹ï¼Œå½“å‰å¸§é©±åŠ¨éšå¼å…³é”®ç‚¹
            out = adef_wrapper.warp_decode(f_s, x_s, x_d_i_new)    
            I_p_i = adef_wrapper.parse_output(out['out'])[0]  # 512x512x3, uint8   ç”Ÿæˆçš„å›¾åƒï¼ˆå¯æ˜¾ç¤ºçš„é‚£ç§0~255ï¼‰
            I_p_lst.append(I_p_i)         # å›¾åƒåˆ—è¡¨ï¼ˆè¿˜æ²¡ç²˜è´´å›å®Œæ•´åŸå›¾ç©ºé—´ï¼‰

        # save the animated result       ä¿å­˜ç»“æœ 
        mkdir(args.output_dir)
        temp_video = osp.join(args.output_dir, f'{basename(image)}_{basename(audio)}_temp.mp4')
        images2video(I_p_lst, wfp=temp_video, fps=inf_cfg.output_fps)
        final_video = osp.join(args.output_dir, f'{basename(image)}_{basename(audio)}.mp4')
        add_audio_to_video(temp_video, audio, final_video)     # æ·»åŠ éŸ³è½¨
    return None

if __name__ == "__main__":
    # transf_train()
    infer()
